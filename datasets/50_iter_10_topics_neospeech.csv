date,url,title,contents,author,topic
2016-04-06,http://blog.neospeech.com/2016/04/06/blind-can-see-photos-on-facebook-and-twitter/,Blind People Can Now “See” Photos On Facebook And Twitter,"Less than a week apart, two giants of social media announced new text-to-speech based features aimed at increasing accessibility on their websites. Both have similar purposes, and may seem like small additions. However, it is a huge step in making the internet more accessible to people with disabilities. People with vision impairments will now be able to “see”, or more accurately, hear photos on Facebook and Twitter. It’s no secret that Facebook and Twitter are two of the most popular websites on the internet. They’re the 3  and 11  most visited websites in the world, respectively. The practice sharing information with friends and families through these social networking sites have become a norm in today’s society. Social networking, in a sense, is like real life put onto a computer screen. This is great for most people, however, it becomes very complicated for people with vision disabilities, or blind people. It is estimated that there are 285 million people worldwide who are visually impaired. With social media becoming a bigger and bigger part of our daily lives, it’s important to keep these websites accessible to these people. Some of you might be asking, “So how do blind people navigate the internet?” The answer is screen readers. Screen readers are applications that anyone can download onto their computers. Once installed, the screen reader will read out all of the text that is on the screen. They will read out the text from any program you are running or any website you are on. This technology applies to social media as well. Screen readers are able to read out who is posting and everything they wrote. However, what’s missing here is that a significant portion of social media posts aren’t text-based, they’re pictures. Screen readers can’t say anything when there isn’t any text. Last week, Twitter broke the news that they developed a feature that would allow screen readers to describe images to users. And just this week, Facebook announced a similar feature. The short way of saying it is that screen readers will now be able to describe images to users so that they can gain an understanding of the picture. However, the methods that Facebook and Twitter use to achieve this are quite different, and it is important to understand how each one works so you can be sure that the images you post will be accessible to those with disabilities. If you want visually impaired people to be able to “see” your photos on Twitter, then it’s up to you to determine what they’re going to hear. With Twitter, users can add captions to 
their photos. This caption will be recognized by a screen reader and converted to speech. Thankfully, Twitter doesn’t limit you to just 140 characters. Instead, you have up to 420 characters to describe your image. This feature is only available on the Twitter iOS and Android apps. To add a description, just tap the pen icon on the image after you have uploaded it. Twitter thanks their users for coming up with this idea. When Twitter’s CEO Jack Dorsey asked developers about which features they wanted Twitter to have, adding descriptions to images was the fourth most requested feature. Facebook, like Twitter, also uses alternative text on images that are then converted to speech by screen readers. However, the big difference here is that users don’t write the image descriptions. Instead, Facebook does it themselves. “Automatic alternative text” is a tool that recognizes objects and uses that to describe an images. For example, if I posted a picture of myself standing next to a car, then Facebook’s automatic alternative text tool might describe the image as “a man and a blue car”. People using a screen reader would then hear that description as they are browsing Facebook. Facebook’s new feature is currently only available on their iOS app and only in English. And as of now, Facebook’s tool can recognize about 100 different types of objects and scenery. Here’s a video showing Facebook’s new feature in action: Text-to-speech technology has long been used to help make the world around us more accessible to everyone. We’re excited to see two such popular websites using this technology for that purpose! If you’re interested in having your screen read out loud to you, then check out our   that lets you type any text you wish to hear from any of our voices to see which one is right for you! Are these features that you’re going to use? What other accessibility features do you wish Facebook or Twitter offered? Can you think of something else that text-to-speech may help with? Let us know in the comments! To learn more about the different areas in which Text-to-Speech technology can be used, visit our   page. And to learn more about the products we offer, visit our   page. If you’re interested in adding Text-to-Speech software to your application or would like to learn more about TTS, please fill out our   form and one of our friendly team members will be happy to help.",neoadmin,2
2016-04-20,http://blog.neospeech.com/2016/04/20/neospeech-bitcode-support/,"Good News, Bitcode Is Now Supported In Our VoiceText iOS Embedded SDK!","If you’re an app developer, chances are you cheered for joy upon reading that title. If you’re not an app developer, then you probably have no idea what this means. However, there’s still reason for you to celebrate as well.   support is great news for both developers and Apple product users. Apple unveiled Bitcode at Apple’s Worldwide Developer Conference in June of last year. It was announced that Bitcode would “allow the App Store to re-optimize apps for each kind of device before they’re delivered to the user”. Apps would also automatically accommodate for updates released by Apple. In short, Bitcode is able to optimize an application to be used by the intended device. To give you an idea, the Facebook app on an iPhone is not entirely the same as the Facebook app on an iPad. Since these are different devices, the app needs different resources to be able to run on each particular device, so the executable architecture of the app is different. In the days before Bitcode, if a developer wanted their app to be compatible with all Apple devices, then they would have to make sure that their app contained all of the resources needed to run on each device. Then, whenever an end-user downloads the app, it would contain the executable architecture for their device and every other device, making the file size of the app much larger. With Bitcode, the iTunes store can automatically optimize an app for a specific device. When the app gets downloaded, it’s only downloaded with the executable architecture for that particular device. This is good for developers because with Bitcode, all they have to do now is upload a full version of their app to iTunes. Then, the iTunes store will do the rest of the work and create a version of the app that is compatible for one particular device, such as the iPhone, iPad, and Apple Watch. Another time-saving benefit of Bitcode is that whenever Apple releases an update, iTunes will automatically update the app too. Before, if Apple released a new update of the iPhone, then a developer would need to reconfigure their app so it would still work properly. Now this will be done automatically by iTunes. There’s two reasons why you should be happy here. First, because the app you download will only have the executable architecture for your device, it means the size of the app will be smaller. Apps with Bitcode will require less of your precious memory to store. This also means that it will download faster. The second reason why you should be happy is that apps with Bitcode will automatically update with each Apple update. No longer will end-users have to worry about their apps becoming out of date and incompatible with their device. Glad you asked. Several apps use NeoSpeech today. Popular uses include screen reading, eLearning, navigation, and alerts. Developers have been using NeoSpeech’s Text-to-Speech technology to create breakthrough apps that make information more accessible to all, especially users who are visually impaired. We wouldn’t be surprised if you’ve already heard our voices on your Apple device. 
You can also check out some apps that we’ve released ourselves. Our Spokesperson app lets you type in messages that become converted to speech with our voices! You can save messages and quickly tap them to have it read aloud. Our Newspeak app reads out news stories to you, so you can keep up with current events without having to strain your eyes by reading on a tiny screen. This app can be especially useful for when you’re driving and wanting to listen to the news! Do you use any apps that have text-to-speech features? Are there any apps out there that you think should have text-to-speech? Let us know in the comments! To learn more about the different areas in which Text-to-Speech technology can be used, visit our   page. And to learn more about the products we offer, visit our   page. If you’re interested in adding Text-to-Speech software to your application or would like to learn more about TTS, please fill out our   form and one of our friendly team members will be happy to help.",neoadmin,4
2016-04-04,http://blog.neospeech.com/2016/04/04/cvaa-compliance-technologies-texttospeech-deliver-audio-alerts/,CVAA Compliance – New Technologies Using Text-To-Speech To Deliver Audio Alerts,"Text-to-speech, a very beneficial feature for people with disabilities, has widely been seen as only that; a neat feature. But as of late last year, text-to-speech became more than that. It became a requirement by law. Thanks to emerging technologies and a proactive approach to improve accessibility, the U.S. passed the 21  Century Communications and Video Accessibility Act (CVAA) in 2010. The purpose of this act was to require television broadcast stations and video programming providers to use these technologies to make information more accessible to people with disabilities, such as people who are blind or have a visual impairment. There were several rules within this act that went into effect over time. One of the latest, known as the Audible Crawl rule, went into effect on November 30, 2015. The FCC are “requiring the use of a secondary audio stream to convey televised emergency information aurally, when such information is conveyed visually during programming other than newscasts, for example, in an on-screen crawl.” So what exactly does that mean? You know those emergency alerts you’d see on TV scrolling across the bottom of the screen? They’re the ones that often provide weather updates or AMBER alerts. This new rule is saying that broadcasters and providers must have an audio stream which converts the text to speech. This is where text-to-speech software, or a text reader, comes into play. The TV station or programming provider must have a device in place that will read out the alert the moment they receive it. The reason for the urgency is because when a TV station receives an emergency alert, it gets instantly broadcasted over the air. A TV station can’t delay an alert from going out. Therefore, a programming provider or television station can’t employ a voice actor 24-hours a day and have them constantly ready to announce an alert. A text-to-speech synthesizer is a must have for these companies. It needs to be in place and always ready to convert a text alert into speech. (The AIM-100 automates the text-to-speech conversion process) Thankfully, it’s getting easier and easier for TV stations and program providers to be in compliance with the new FCC regulation. Software and devices that are able to analyze and read aloud the text from emergency alerts are being released at a rapid rate. One such product was announced last week. Enco will be unveiling their new   (AIM-100) next month. TV stations simply hook up this device to their system and let it do the rest of the work. The AIM-100 will identify incoming text files and automatically convert it to audio as required by the FCC. NeoSpeech voices will be available on the AIM-100. There are many other devices like this on the market, and this also gives TV stations and program providers customization options. Broadcasters can shop around find the best sounding and most natural text-to-speech voices (we wouldn’t be surprised if you’ve heard any of our NeoSpeech voices already). Gone are the days when TV viewers had to sit through and listen to those terrible “robot” voices during an emergency alert. This is where things get exciting. While text-to-speech is being used for SAP (second audio program) voices, it can be used for much more. Text-to-speech voices are becoming so realistic that soon providers will be using them for much more than just emergency alerts. Broadcasters can allow the TTS voices to announce anything such as the news, weather updates, sport reports, and multiple-language programming. This isn’t a futuristic idea, it’s already possible today! Devices such as the AIM-100 are more than capable of handling these functions. As we at NeoSpeech continue to push the limits of high-quality text-to-speech voices, we look forward to hearing our voices more and more in the world around us as companies continue to improve accessibility in each of our daily lives.  Want us to prove how realistic our voices are? Go to our   and type any text you wish to be spoken by any of our 30+ voices in any of our 7 available languages! This blog post just covered one small part of the CVAA. There are many other regulations that must be abided by. Thankfully, we’ve covered every part of the CVAA already. Are you from a television station or a multi-channel video programming distributor (MVPD)? Then look no further than here to learn how you can remain in compliance with the FCC. Read any of our blogs on the CVAA below! If you’re a broadcaster or provider, what are you doing to make sure you are compliant with the CVAA? As for everyone else, have you noticed any of these accessibility improvements? Let us know in the comments! To learn more about the different areas in which Text-to-Speech technology can be used, visit our   page. And to learn more about the products we offer, visit our   page. If you’re interested in adding Text-to-Speech software to your application or would like to learn more about TTS, please fill out our   form and one of our friendly team members will be happy to help.",neoadmin,4
2016-04-07,http://blog.neospeech.com/2016/04/07/voices-added/,NeoSpeech Releases Two New Voices!,"We’re proud to announce two new additions to our ever-growing list of voices, including our first ever male Canadian-French voice! Leo and Beth are now fully available in all of our product offerings, giving users and developers more customization in their text-to-speech applications. Leo, our first male French-Canadian voice, and Beth, our latest US English voice, raise the standards for high-quality speech synthesizers. We don’t like to brag often, but these are both outstanding voices. Leo perfectly captures the allure of the French-Canadian accent. His voice strikes the delicate balance between a deep yet elegant tone. Leo’s voice has been described as friendly and professional, making his voice perfect for business use. Make Leo’s voice the first thing your customer hears when they call your company and they’ll be instantly hooked. In addition to using Leo for IVR (Interactive Voice Response) and CTI (Computer Telephony Integration) purposes, his smooth and realistic voice makes him ideal for announcement systems, broadcasting, eReading, and personal use. Now, we wouldn’t just talk up Leo’s voice this much without backing it up.   and see what our excitement is about! You can also put Leo to work in the demo section of our   by selecting his voice and typing any text you wish for him to speak! You can also read our press release on Leo  ! Beth is our fourth female, and sixth overall US English voice! This gives our customers a wide range of variety in selecting just the right English voice for their applications. What makes Beth so special is the attracting and warming tone of her voice. These traits are often missing from synthesized speech, but Beth’s voice has the undeniable ability to connect with her audience. This personal connection that Beth achieves makes her voice ideal for education, language learning, screen reading, health monitors, medical devices, and of course, personal use. Again, we wouldn’t throw all of these claims at you without proving it. You can listen to Beth’s voice yourself and allow her to read any text you type by selecting her voice on the demo section of our  . Ever heard the phrase “slow and steady wins the race”? Well, that’s the approach we take when developing our speech engines. As we discussed in our blog post “ ,” we use the USS technique which takes much more time, but as of today producers the best quality voice. It takes us about a full year of hard work from start to finish to create a new voice using this technique, but the superior quality of our voices definitely makes the effort worth it to us! And we think you’ll agree when you experience our text-to-speech technology. You bet! We’re expanding our line by adding new languages. Plus, we’re also adding voices to languages we already support to give you more customization. We’re working tirelessly to continue to be the leading provider of text-to-speech technology, and we can’t wait to introduce you to our new voices, or better yet, to have our new voices introduce themselves to you! Did you listen to Leo’s or Beth’s voice? What do you think of them? What sort of technologies or applications would you like to hear these voices in? Let us know in the comments! To learn more about the different areas in which Text-to-Speech technology can be used, visit our   page. And to learn more about the products we offer, visit our   page. If you’re interested in adding Text-to-Speech software to your application or would like to learn more about TTS, please fill out our   form and one of our friendly team members will be happy to help.",neoadmin,6
2016-03-18,http://blog.neospeech.com/2016/03/18/wikipedia-offer-texttospeech-synthesis/,Wikipedia To Offer Text-to-Speech Synthesis,"The seventh most popular website in the world is now working to create the world’s first crowdsourced text-to-speech platform. This new feature will allow users to have the text from Wikipedia pages read aloud to them. Wikipedia is the latest among a growing number of companies making their websites more accessible to users with disabilities who may have trouble reading text on their screens. This is big news considering a   estimated that a quarter of Wikipedia users, or 125 million per month, need or prefer their text read aloud to them. The KTH Royal Institute of Technology in Sweden is currently developing the platform. The text-to-speech servers will be hosted by Wikipedia, meaning users won’t have to download any software to use the feature. In addition to this service, Wikipedia will also make the speech synthesis platform open source. Websites will be able to easily download and use the software for their own web pages. Individuals will also be able to download the platform for personal use. The goal is to have English, Swedish, and Arabic speech engines ready by 2017. Wikipedia plans on eventually offering synthesized speech for each of the 281 languages that are supported by the online encyclopedia. Eventually, users will also be able to report words that are pronounced incorrectly in an effort to correct the speech. This crowdsourcing element falls in line with Wikipedia’s defining characteristic. Since 2001, Wikipedia has grown its database to an astronomical size thanks to users freely contributing content, writing articles, and correcting mistakes. While this will be the first crowd funded TTS engine, it won’t be the first open source TTS engine. Companies like Google and Microsoft offer text-to-speech synthesis for free as well. This has earned those companies praise for offering tools to make the internet and computers more accessible. The main drawback with free TTS engines are that they are commonly considered lower quality and sound less realistic. We’re excited to see Wikipedia roll out this feature in the future and eventually support almost 300 languages, which is nothing short of amazing! We hope other websites follow Wikipedia’s lead and work to make the entire internet more accessible to everyone. Would you use Wikipedia’s text-to-speech feature? Which website would you like to have TTS functionality? Let us know in the comments! To learn more about the different areas in which Text-to-Speech technology can be used, visit our   page. And to learn more about the products we offer, visit our   page. If you’re interested in adding Text-to-Speech software to your application or would like to learn more about TTS, please fill out our   form and one of our friendly team members will be happy to help.",neoadmin,4
2016-04-13,http://blog.neospeech.com/2016/04/13/ivr-customer-self-service/,IVR: The Future of Customer Self-Service,"“Customers don’t expect you to be perfect. They do expect you to fix things when they go wrong.” This quote stands very true in the business world. These days, consumers expect to have their answers immediately. There’s no time to waste if a company wants to provide top level customer service. Interactive Voice Response, or IVR, is a technology that transforms your customer call center into an efficient customer self-service system. Have you ever called a company and heard a pre-recorded message greeting you and then asking you to press a certain number? That’s IVR. However, that form of IVR is becoming outdated. Text-to-speech and other new technologies are emerging that give companies the competitive advantage when it comes to their customer service. IVR systems are becoming more advanced are appear to be able to have conversations with callers. So how do you keep up with the latest trends and technologies concerning IVR? Thankfully,   aimed at bringing you up to speed with the newest technologies that will take your customer service system to the next level. This course is geared towards teaching you how to use these technologies to create a system that lets your customers interact with your company exactly how they want to, and with the best possible outcomes. Large companies often get hundreds or even thousands of customer service calls a day. The old way to meet this demand was to employ a staff of dedicated customer service representatives who would field these calls throughout the day. Times have changed though. The costs of maintaining a customer service call center is high, and companies are moving away from this approach. But consumer’s demands for information are increasing, and they want it faster. Companies have turned to IVR to solve these problems. IVR is a technology that lets intelligent computer applications communicate with your customers. IVR has been utilizing voice recognition and text-to-speech technology to provide callers with a better experience. Having a good text-to-speech engine is important because having a high quality voice is vital to keeping customer satisfaction high. The perfect IVR system can clearly communicate with the caller and give them the answers they’re looking for. And they can do this in a timely manner. Unfortunately, it can be difficult to implement a perfect IVR system for your company. That’s why Speech Tech Magazine is hosting their five-part web series – Beyond IVR: A Crash Course in the Next Generation of Customer Self-Service. Each webinar lasts 50 minutes. And the five webinars will take place over the course of 5 weeks, with the first webinar coming up next Tuesday, April 19. It’s not too late to sign up if you’re interested, which you can do  ! Take a look at what each lesson has to offer: One of the main goals of these webinars is to help give you an understanding of the future of voice apps and speech technology, and what they mean for IVR. Text-to-speech synthesis is vital to the success of an IVR system. You should want the voice answering your phone calls to be the best and most natural sounding voice. After all, it’s the voice of your company! The webinar will also focus on other topics such as how to improve user experience, crafting the perfect messaging, utilizing “context cookies”, and delivering the best experience to your customers effortlessly. We at NeoSpeech recommend any business out there to enroll in this web series. No matter how big or small your company is, there is an IVR system out there that is right for you. Several companies of all sizes have turned to NeoSpeech to provide the highest quality text-to-speech voices for their IVR systems. While having a great voice for your IVR system is a huge first step, it is also important to know how to set up your IVR system so it can say the right things at the right times. This is why we believe this web series is a great use of any company’s time. What kind of IVR system do you have in place? What are the newest trends in this field that you are starting to see? Let us know in the comments! To learn more about the different areas in which Text-to-Speech technology can be used, visit our   page. And to learn more about the products we offer, visit our   page. If you’re interested in adding Text-to-Speech software to your application or would like to learn more about TTS, please fill out our   form and one of our friendly team members will be happy to help.",neoadmin,1
2016-04-14,http://blog.neospeech.com/2016/04/14/emergency-alert-notification-high-quality-voice/,Why An Emergency Alert Notification Needs A High Quality Voice,"We’ve all heard it before. Your TV or radio begins buzzing and beeping with an all-too-familiar tone. It’s an emergency alert. You listen up for the information that’s about to be read to you. It could be a weather notification, or an AMBER alert. Either way, you listen carefully to the message, because the information could be important to you. Most of the time, the next thing you hear is a synthesized voice. It’s not actually someone reading out the message; rather, it’s a text-to-speech engine that is able to convert the text alert into an audible message that gets broadcasted to you. Unfortunately, residents in Ontario, Canada fell victim to a poorly executed text-to-speech system. Last month, an AMBER alert came on after a running car with a baby inside was stolen. The alert was difficult to understand as the voice was not high quality. Then, the message ended with, “If observed, call September 1 , 2001”. In this case, the system failed to read out “9-1-1” and instead read it out as a date. Another incident occurred before that after a possible abduction. The TTS voice so poorly announced the name of the city (Orillia, Ontario) that some people didn’t even know what had been said. With the recent advances in text-to-speech technology, it’s shocking to see such an issue still exist. TTS is increasingly being used for announcing important information. In these situations where every detail could matter, it is vital to have a clear and correct, natural sounding text-to-speech voice announce these alerts. In addition to these emergency alerts we hear on the TV or radio, text-to-speech technology has also been used to announce alerts through computer networks and phone lines. For example, let’s say a severe snow storm has dumped 10 feet of snow around a city, and school is scheduled to open in a few hours. There’s no time to call each family individually and tell them school is closed. Instead, many school districts have a system in place that lets you type in the message, and then it automatically calls each family and reads aloud your message using text-to-speech. In this case, it’s telling them that the school will be closed for the day. Fire stations use TTS to announce incoming calls so firefighters can get the information as they rush to put their gear on. Businesses use it to announce meetings, product recalls, instructions, or other timely information. The point is that is there are thousands of examples of when information needs to be relayed in an instant. Using TTS technology is the best way to address this issue. However, it is important to realize that this may not be the best time to go cheap. In these cases, you want to have the highest quality voice. There can be a variety of reasons to send out an alert, but most alerts contain information that could be important to one’s well being, and are urgent. In these situations, many people could be affected by the alert you are sending out. So, you better make sure that they fully understand the information you are sending them. Going back to the previous example, what if someone from Orillia could have spotted the abductor after the alert was broadcasted, only they didn’t because the quality of the voice was so bad that they didn’t have any details to go off of? This would be a tragedy. Emergencies are the most important times for clear communication. TTS technology is great for creating audio alerts effortlessly and instantly. We strongly recommend anyone wanting to use this technology to find the best quality solutions. Here are a few tips we have so you can be sure that your system is flawless: As we mentioned earlier, TTS is being used for more than TV or radio alerts. Businesses, schools, computer programs, airports, fire stations, public transit stations, hospitals and medical devices all use text-to-speech to announce alerts. In emergency situations, we implore you to choose to best voice you can find. Our NeoSpeech voices have been used for alert notifications for years, have you heard any? What sort of TTS engine do you use to send out alerts? Let us know in the comments! To learn more about the different areas in which Text-to-Speech technology can be used, visit our   page. And to learn more about the products we offer, visit our   page. If you’re interested in adding Text-to-Speech software to your application or would like to learn more about TTS, please fill out our   form and one of our friendly team members will be happy to help.",neoadmin,1
2016-05-04,http://blog.neospeech.com/2016/05/04/taiwanese-texttospeech-voice/,"Meet Yafang, Our First Taiwanese Text-to-Speech Voice","We’re excited to announce yet another new release. Yafang, a female Taiwanese voice, is now available to purchase for personal or commercial use. This comes after the release of Beth, a US English female voice, and  . Yafang is our first Taiwanese voice. Adding Taiwanese gives customers more opportunity to reach new audiences with NeoSpeech. Other languages include English (US and UK), Mexican Spanish, Canadian-French, Chinese, Japanese, and Korean. A proper introduction is in order, so how about we let Yafang introduce herself! Click   to listen for yourself what a high quality Taiwanese text-to-speech voice should sound like. Yafang speaks Taiwanese Mandarin. This is considered a dialect of standard Mandarin spoken in mainland China. It is the official language of Taiwan. However, it wasn’t always that way. From 1890 to 1945 during the Japanese colonial occupation, Japanese was the official language of Taiwan and was taught in schools. At the conclusion of World War II in 1945, the Republic of China took control of Taiwan from Japan and introduced Mandarin as the official language. At this point, schools were required to teach mandarin in schools. However, the influences of Japanese as well as other popular languages in Taiwan such as Taiwanese Hokkien made the Mandarin spoken in Taiwan have noticeable differences to the Mandarin spoken in China. This includes differences in pronunciation, grammar, and vocabulary. Most of the Taiwan’s population of 23 million people can speak Taiwanese Mandarin fluently. Yafang is one of our most natural sounding text-to-speech voices to date. She can handle any text, long or short, and convert it into a high quality sound file. Her speech flows perfectly together. This makes Yafang perfect for any commercial purposes. If you’re running a call center using IVR/CTI technology and you expect Taiwanese callers, then you should definitely consider integrating Yafang into your system. Yafang can also be used for audio publishing, broadcasting, eLearning, training videos, and navigation systems, among several other uses! NeoSpeech offers over 30 voices in 7 different languages. What separates NeoSpeech from other text-to-speech providers is the quality of our voices. We strive to produce high quality, natural sounding text-to-speech voices. Yafang, as well as our other voices, was created using the Unit Selection Synthesis (USS) method. As of today, this method produces the most natural sounding synthesized speech. We use a least 20 hours of recorded audio to make these voices to ensure they produce the correct sounds each time they’re talking. You can read more about how we create our voices in  ? Did you listen to Yafang’s introduction? What did you think of her voice? What other languages would you like us to produce? Let us know in the comments! To learn more about the different areas in which Text-to-Speech technology can be used, visit our   page. And to learn more about the products we offer, visit our   page. If you’re interested in adding Text-to-Speech software to your application or would like to learn more about TTS, please fill out our   form and one of our friendly team members will be happy to help.",neoadmin,6
2016-04-27,http://blog.neospeech.com/2016/04/27/standalone-texttospeech-engine/,What Can You Do With A Stand-Alone Text-to-Speech Engine?,"The quality of speech synthesis has come a long way in recent years. Text-to-speech technology was once defined by the robotic and unnatural sounding nature of their voices. Today, it can be almost impossible to tell the difference between a synthesized voice and a real one. In addition to improvements to text-to-speech’s quality, the availability of this technology has also increased. With the rise of computers over the last few decades, applications and programs have been developed with text-to-speech functionality. With this feature, applications are able to convert text into speech. There are several types of applications that use text-to-speech. There’s eLearning, screen reading, alerts, and many other types of applications. These apps use text-to-speech as an extra feature to help support the main purpose of the app, such as to educate you. But what about stand-alone text-to-speech applications? Their only purpose is to convert the text you enter into audible speech. How do these work? And is there much use for them? While every application is different, usually how it works is that when you open the app, there’s a box where you can input text. Once you enter in all the text you want converted to speech, you select an ‘enter’ or ‘speak’ button and the rest will be taken care of for you. Most of these apps also give you some customization option such as selecting the voice, editing tempo, and changing the pitch. These apps can either be installed on your computer or accessed online through a text-to-speech provider. This type of application can be inexpensive and versatile. However, not many people know about all the things that they could do with this type of application at their disposal. In this blog post, we’ll show you how anyone can benefit from having a stand-alone text-to-speech engine and list out top 5 uses for one. A big challenge with learning a new language is figuring about how certain letters and words are supposed to be pronounced. Thankfully, the quality of text-to-speech engines have improved so much that their voices can be excellent teachers for us. An easy way to learn how to say something in a new language is to enter in the word or phrase you want to learn and listen to it a few times. This is a good way to learn a new language without the high price of a language teaching application. Are you a big traveler? This would also be a fantastic tool for learning a few essential phrases that’ll help get you through your next trip abroad. Learning how to order food and say “please” and “thank you” is easy this way and could greatly benefit your trip. This is a relatively unknown yet amazingly beneficial use of a text-to-speech engine. Proof-reading is something that just about everyone needs help with. It’s much easier to catch mistakes when someone else reads your writing or when you read it out loud. Putting your writing in a text-to-speech application essentially takes care of both! You’ll be able to hear your mistakes. People often glance over typos when proof-reading their own papers. Converting your text into audio will make your typos obvious when you hear them. In addition to proof-reading, did you know that text-to-speech can also improve your writing skills? Last week,   detailing how a text-to-speech converter can improve your business writing skills. When you listen to your paper or report, you’ll be able to tell if it’s flowing the way you want it to. You’ll hear it when a word or sentence seems out of place. This allows you to pinpoint the right words to use and to make sure no parts of your paper are out of order. This will increase the effectiveness of your writing. Much of our time spent on computers is reading. We read emails, news stories, blogs, and social media, among several others. There’s a few reasons why it might be better to have this information read aloud to you. First, reading off of a computer screen can strain your eyes. Symptoms of computer vision syndrome include headaches, blurred vision, dry eyes, and neck/shoulder pain. Simply copy and paste the content you want to read into your text-to-speech program and it’ll read you the information. Another benefit is the ability to multitask. When you have a text-to-speech app reading you a news story, you’ll be free to perform other tasks while still listening and taking in the information from the story. Many text-to-speech applications let you save and export audio files. Whenever you input text and convert it into speech, you’ll be given the option to save that audio file to your hard drive, where you’ll have access to it anytime. This is beneficial for any of the examples we just mentioned. Whenever you’re too busy to listen through an entire story or report, you can simply create the audio file and save it for later. You can also send the audio file to any other devices you own such as a smart phone. This will allow you to hear your text-to-speech audio files on the go! You can also create messages in advance using a text-to-speech program. If you rely on these programs to communicate, simply enter a common greeting or introduction message so you can play it whenever you meet someone new. Other uses include creating a voicemail response or creating an instructional tutorial. There’s lots a uses for text-to-speech, and it’s a great tool to have when you can’t always provide your own voice to deliver messages. At NeoSpeech, we offer our VoiceText Editor to anyone wanting a stand-alone text-to-speech engine. Once installed, you have total freedom to customize speech and sounds to your liking. We also offer our  , which means you won’t have to download anything. All you would need is an internet connection. Do you have a stand-alone text-to-speech application? What do you use it for? Let us know in the comments! To learn more about the different areas in which Text-to-Speech technology can be used, visit our   page. And to learn more about the products we offer, visit our   page. If you’re interested in adding Text-to-Speech software to your application or would like to learn more about TTS, please fill out our   form and one of our friendly team members will be happy to help.",neoadmin,6
2016-03-10,http://blog.neospeech.com/2016/03/10/texttospeech-ransomware/,Text-to-Speech Now Being Used By Ransomware,"One of the main purposes of text-to-speech technology is to improve accessibility, which in turn improves the world we live in for the greater good. However, last week it was discovered that TTS functionality is being used by cybercriminals for malicious purposes.  is a type of malware that blocks access to the infected computer unless a ransom is paid. Ransomware usually infects a computer by hiding in a file that the user downloads from the internet, known as a trojan. Once downloaded, it will encrypt all of your files and lock you out of your computer. A pop-up will appear informing you that you have a certain amount of time to pay a ransom in order to receive a code that will decrypt your files and unlock your computer. The pop-up contains instructions on how to pay the ransom, which is usually paid in bitcoin. However, after the time has expired, the code is erased permanently. This essentially keeps you locked out of your files forever. Now, a new ransomware called “Cerber” uses text-to-speech to read aloud the demands of the hacker. Experts say that Cerber is very effective and there is no known way of decrypting the files without paying the ransom. Here is an example of what a Cerber pop-up looks like: There is still a lot unknown about Cerber, however, experts also say that you should not pay the fee in the event that this happens to you. Cybercriminals have no obligations to send you the code to unlock your computer after you’ve paid them. There are cases of the hackers sending the codes, but there are many more cases when they don’t. In another development, security firm Palo Alto Networks confirmed this past weekend the first cases of  . This means that devices running on OS X, or Apple computers, are vulnerable to ransomware. While we are proud of the strides that speech technology has made in recent years, this is unfortunately another example of new technologies being used for malicious purposes. We’d prefer you use text-to-speech technology on your own accord, therefore, we advise everyone to exercise safe browsing practices when using the internet. Do not download files from unknown sources and be cautious of suspicious links. It is also a good practice to periodically back up your computer with an external hard drive. Have you ever had any experience with ransomware? How do you ensure your computer is safe? Let us know in the comments! To learn more about the different areas in which Text-to-Speech technology can be used, visit our   page. And to learn more about the products we offer, visit our   page. If you’re interested in adding Text-to-Speech software to your application or would like to learn more about TTS, please fill out our   form and one of our friendly team members will be happy to help.",neoadmin,0
2016-03-15,http://blog.neospeech.com/2016/03/15/top-5-stephen-hawking-quotes/,Top 5 Stephen Hawking Quotes and Why They Inspire us,"It’s been over 50 years since Stephen Hawking was diagnosed with ALS. The young student was told that he’d become completely paralyzed and only live for two more years. But in the last 50 years, Stephen Hawking has continued to make a tremendous impact in the scientific community, and in each of our daily lives. Over time, the disease slowly took away Hawking’s control over his muscles. His speech became hard to understand. He was confined to a wheelchair. In 1985, he had a tracheotomy which permanently removed his ability to speak. Thanks to advances in technology, especially text-to-speech, Hawking had a way to continue communicating with the world. And with that power, he taught us lessons about the world around us, and about ourselves. Over the past 50 years, Stephen Hawking has given us a collection of inspiring quotes on life, work, and love. Speech is indeed a powerful tool we have. And thanks to speech technology, we have given a voice to those who didn’t have one before. At one point, Stephen Hawking even used our NeoSpeech VoiceText engine and  ! In this post, we’ll take a look at our top 5 favorite Stephen Hawking quotes, and discuss how each of these quotes inspire us in a different way. This is a quote coming from someone whose life many would consider tragic. However, Hawking doesn’t see it that way. Instead, he takes what life hands him and gives it a light-hearted chuckle. With this quote, Hawking is not trying to tell the world that nothing in life is tragic. In fact, he’s almost admitting that a lot in life is tragic. The difference, though, is your personal attitude.  Hawking is saying that you have to look on the bright side, to be optimistic. You can either be a glass half-empty, or a glass half-full kind of person. This quote inspires us because it reminds us that life isn’t always going to be fair, and that it doesn’t have to be the end of the world when something does go wrong. Giving up may be the easy thing to do, but that doesn’t mean it’s the right thing to do. Enjoy life, and don’t take things too seriously. Just give it a laugh. This quote stands out to anyone who has ever wanted to say something but couldn’t. Maybe it was because you were shy, or maybe it was because you never had the opportunity to speak up. Stephen Hawking reminds us though that just because we might not speak sometimes, doesn’t mean that we don’t have a voice. Being quiet is not usually considered a desirable trait. Successful people are expected to be talkative and charismatic. This is expected professionally and socially. This often leaves quiet people feeling overlooked and bad about themselves. This quote by Hawking goes two ways. First, he is reminding everyone out there that just because someone is being quiet, it doesn’t mean that they don’t have anything to say. Try not to overlook the quiet ones. They might just have the answer you’re looking for, or the idea that’ll spark your mind. And second, Hawking is assuring us that there is nobility in being quiet. People who are quiet can still have a loud mind, which can still lead to successful lives. On the surface, this quote is a life lesson to people with disabilities to never give up. However, this quote can be equally inspiring to anyone who reads it. When Hawking said this, he was speaking out to people with disabilities. The message was clear. Just because life took something away from you, it does not mean that your abilities to do other things are hindered. He urges us to keep our heads up and to focus on the things that are still possible. While I don’t personally have a disability, this quote speaks out to me as well. It inspires me because it reminds me that even though I wasn’t able to achieve my wildest dreams (I wanted to be a baseball player), I can still achieve great things with the talents and abilities I have. Bottom line, don’t give up. There are so many things in life that you can do. And always keep your spirits up. This quote inspires us to keep asking questions and to keep learning. Learning is a never-ending task. We all learn new things every day. However, many people don’t see it that way. Many of us tend to think that learning is done once we are finished with school and even celebrate that belief. Hawking is telling us that holding on to that child-like curiosity is a good thing. He’s telling us that that is what led him to find success through his scientific breakthroughs. He didn’t necessarily just pick a topic and master it immediately. He kept asking questions and every now and then he found some answers. There’s also a bit of humbleness in this quote. Hawking is essentially saying that he isn’t a genius. Rather, he’s saying that he just has the mindset of a child. This is inspiring because not all of us can be geniuses, but all of us can keep asking the “how” and “why” questions and search for the answers we are looking for. This quote is inspiring because it reminds us all of the unique and powerful ability we as people have. With talking and listening, Hawking says that we have managed to build the impossible. People have achieved amazing feats over the years, and Hawking tells us that it is all because of communication. With this quote, Hawking also delivers a warning. He says that our greatest failures have come about due to a lack of communication. While failures such as conflicts have been common in human history, Hawking says it doesn’t have to always be this way. We have the power to communicate. As long as we put that ability to use, we can make all of our greatest hopes a reality in the future. This is inspiring because it reminds us that we all have the power to change the world for the better. It’s easy to take speech for granted. It is such a common part of our daily lives that we don’t tend to think about the impact our words can have. This is why we’ve seen a boom in the development of speech technology over the last few decades. With this technology we are able to give voices to both people and to computers. This has made communication more accessible. It has made us more connected with each other, and with technology. Stephen Hawking wasn’t supposed to utter one single word over the past 50 years. But thanks to his determination, and the help of text-to-speech technology, he still has a voice and has used it to make a positive impact on the world we live in. Which of these quotes inspired you the most? Are there any other inspirational quotes that you live by? Let us know in the comments! To learn more about the different areas in which Text-to-Speech technology can be used, visit our   page. And to learn more about the products we offer, visit our   page. If you’re interested in adding Text-to-Speech software to your application or would like to learn more about TTS, please fill out our   form and one of our friendly team members will be happy to help.",neoadmin,9
2016-05-06,http://blog.neospeech.com/2016/05/06/top-10-spoken-languages/,Top 10 Most Spoken Languages In The U.S.,"Pop quiz: what’s the official language of the United States? For those of you who answered “English”, you’ll be surprised to learn that English is not the official language of the U.S. In fact, at the federal level, the U.S. has no official language. Given the United State’s history as a melting pot of cultures and diverse languages, this makes sense. Designating a language (or multiple languages) as an official language means only that language can be used within the government. However, many states within the U.S. have made English their official language. None the less, millions of households across America speak languages other than English in their homes. The colonization of the U.S. from many European countries, as well as immigration throughout the decades from around the globe, has made the U.S. home to a diverse group of speakers. Thanks to research from the  , and  , we can see what the most widely used languages in the U.S. are. Respondents were asked what was the primary language spoken in their household. Here are the top 10 most used languages in the U.S. I’m sure this doesn’t come as a surprise to most of you. English is indeed the de facto language of the U.S. About 80% of households in the U.S. speak English only. English is the primary language used in government, taught in schools, and used in entertainment. The United States inherited English from the British colonization. The influence of the United Kingdom in the U.S. as well as the large amount of people coming from the U.K. during the early days of the U.S. led to the widespread usage of English as a primary language. The prevalence of Spanish in the U.S. can also be traced back to the colonization of America. Spanish was actually the language of the first permanent European settlers. Spain had colonized Florida and several parts of Latin America. Today, immigration from Latin America and Mexico has also led to a boom in the usage of Spanish. In fact, states bordering Mexico have the highest prevalence of Spanish speakers (California, Arizona, New Mexico, and Texas). It has been said that the U.S is the second largest Spanish speaking community in the world, beating Spain and following Mexico. When we say “Chinese language”, we actually mean all the varieties, or dialects, of Chinese. Varieties include Mandarin, Cantonese, Hakka, Taiwanese, and a few others. Collectively, Chinese is the third most spoken language in the U.S. The highest concentrations of Chinese speakers in the U.S. are in California and New York, which boosts large immigrant communities. In fact, it was estimated in 2000 that 40% of all Chinese speakers in the U.S. live in California, which took in scores of immigrants from China over the past couple centuries. France’s early influences in Canada and in Louisiana are responsible for French being the fourth most spoken language in the U.S. It’s not surprising that the highest concentrations of French speaking households are Louisiana (which the U.S. bought from France in the Louisiana Purchase), and in Northeastern states such as Maine. These states border Eastern Canada where Canadian-French is spoken. Tagalog is the national language of the Philippines. However, it is spoken as a first language by only about a quarter of Filipino population, while Filipino is used more widely as a first language. The number of Tagalog speakers in the U.S. increased after the U.S. annexed the Philippines (The Philippines then became an independent country in 1946). Today, most Tagalog speakers in the U.S reside in Hawaii, California, and Nevada. The sixth most spoken language in the U.S., the rise of the Vietnamese population in the U.S. occurred mostly after the communist takeover of South Vietnam after the Vietnam War. Little Saigon in Orange County, California is known for its large Vietnamese community. California is home to the highest percentage of Vietnamese speakers in the U.S. Korean is the seventh most spoken language in the U.S. Like Vietnamese, the Korean population of the U.S. has boomed during the past century. A surge of Korean immigrants came after the Korean War. The states with the largest Korean populations today are California, New York, New Jersey. The usage of German in the U.S. can be traced back to the colonial days, when German was widely spoken throughout some of the colonies, especially Pennsylvania. German was also the most used language in the U.S. (other than English) in the early 1900s. As of today, there are two states in the U.S. where German is the second most spoken language behind English. They are North Dakota, and South Dakota. Like Chinese, there are varieties of Arabic included here. Arabic speakers have been emigrating to the U.S. from Middle East throughout the nation’s history. Many Muslim Americans still use Arabic today for religious purposes. It’s interesting to note that many Arabic speakers already know English upon resettling here, given English’s widespread usage in the Middle East. States with the most Arabic speakers are California, Michigan, New York, and Texas. Rounding out the top 10 is Russian. Russian is still used today in many areas of Alaska, which used to be under Russian control. During the Cold War, many immigrants from the Soviet Union and its satellite nations came to the U.S., increasing the usage of Russian. New York, California, and Washington have the most Russian speakers. – Given the diverse nature of the U.S., it’s important for businesses to understand the prevalence of languages other than English in the U.S. Accommodating languages other than English can lead to businesses reaching more potential customers within the U.S. What language or languages are spoken in your home? What’s your take on the importance of businesses in the U.S. offering multilingual support and services? Let us know in the comments! To learn more about the different areas in which Text-to-Speech technology can be used, visit our   page. And to learn more about the products we offer, visit our   page. If you’re interested in adding Text-to-Speech software to your application or would like to learn more about TTS, please fill out our   form and one of our friendly team members will be happy to help.",neoadmin,2
2016-03-03,http://blog.neospeech.com/2016/03/03/voter-registration-more-accessible-than-ever/,Get Out And Vote! Voter Registration Becoming More Accessible Than Ever,"In 2014, it was estimated that out of the 35 million Americans with disabilities who were eligible to vote, only 15 million voted. That is a voter turnout rate of only 42 percent. Meanwhile, voter turnout for all Americans for the 2012 presidential election was about 55 percent. While these numbers may seem low, they show a slow and steady increase in voter turnout since 2000. In the 1990’s, the federal government determined that voter turnout was too low for their liking. They wanted to get more Americans to the polls on Election Day. The solution? Make voter registration easier and more accessible. Since then, several laws and programs have been put in place to give every American a chance to register to vote. New technologies seem to come out every election cycle that makes the registration and the voting process easier. Today, the ability to register to vote online is available across most of the United States. And even more excitingly, we are seeing states start to use text-to-speech technology to make the voter registration process more accessible for people with disabilities! In this blog post, we’ll take you back through the history of the voter registration process, and how it has rapidly changed since the turn of the century. We’ll also take a look at how TTS technology is being used today and how it could be used down the road in future elections! George Washington famously became the first president of the United States in 1789. However, only a mere 6 percent of the U.S. population were even eligible to vote at the time. The recently ratified U.S. Constitution did not define who was eligible to vote. This gave each individual state the power to make their own eligibility requirements. For the most part, only white men who owned property were eligible to vote. So if you were one of the few who were eligible to vote, how did you register? Well, you didn’t. Back then, you did not register yourself. Instead, legislators were responsible for compiling a list of everyone who was eligible to vote in their jurisdiction. It wasn’t until the early 1800s when states started requiring individuals to register themselves to vote. The reason why states started moving toward this method of registration was to combat voter fraud, such as double voting or using fake names. By the time of the Civil War in the mid-1800s, most white men, regardless of property ownership, were eligible to vote. The right to vote was still withheld from people of color, women, and Native Americans. Over the next 100 years, several laws would be enacted to give these people the right to vote. While the Supreme Court was giving more citizens the right to vote, individual states still had control over the voter registration process. Many used that power to continue discriminating against minorities. Several states required literacy tests and taxes to register to vote in an effort to keep people of color from voting. This continued for years until the   protected voter registration for minorities. This act paved the way for all eligible U.S. citizens to have an equal opportunity to register to vote. Literacy tests and poll taxes were a thing of the past. Fast-forward to the 1990’s. The government was looking for ways to improve low voter turnout numbers. What they decided was that it was necessary to create general standards for the voter registration process throughout the country. After several unsuccessful attempts, the  , or the Motor Voter Act, was signed into law in order to increase the number of registered voters and enhance voter registration. States were now required to make voter registration more accessible to its citizens. Since then, we have seen new technologies being used to make registration easier than ever, including text-to-speech! Unless you’ve registered to vote in the past few years, chances are you haven’t experienced this. You can believe it though! Online voter registration became a reality in 2002 with Arizona pioneering the movement. This was a giant leap for improving accessibility for people with disabilities. Registering to vote became as simple as logging in to your home computer. We still have some time before the entire United States has the opportunity to register to vote online, but as of today, 30 states (plus the District of Columbia) offer online registration! Take a look at the map below to see if your state is one of them. (Florida and Oklahoma have enacted Online Voter Registration Acts, but have yet to implement them) As stated earlier, TTS technology is now being used to help people register to vote! For example, the State of Indiana has added a TTS application to their online registration website. This app reads aloud the text on the computer screen. This is extremely helpful to people with vision disabilities or anyone else who might have difficulties reading their computer screen. TTS is also starting to be used by voting machines! Voters can now have their ballots or the instructions read aloud to them if they are blind or visually impaired. The implementation of TTS technology has helped voting in the United States become more accessible than ever before! Recently, the government has shown an interest in exploring the use of new technologies in the registration and voting processes. The  , or EAC (created by the Help America Vote Act of 2002), has been pushing for more advances like the use of TTS. In 2009 and 2010, the EAC awarded a total 8 million dollars through a competitive grant program aimed at improving voter accessibility, called the Accessible Voting Technology Initiative. It’s safe to assume that technology will continue to change our world at a rapid rate. It hopefully won’t be long until online voter registration is available in all 50 states. As far as TTS goes, anything that is written in text can be converted into speech, and we are already seeing this technology being utilized by online registration systems and by electronic voting machines. We can only speculate as to what voter registration and voting will be like in the future. Who knows? Maybe both of these processes will one day be entirely controlled by our voices, and it would all take place in our home!     To learn more about the different areas in which Text-to-Speech technology can be used, visit our   page. And to learn more about the products we offer, visit our   page. If you’re interested in adding Text-to-Speech software to your application or would like to learn more about TTS, please fill out our   form and one of our friendly team members will be happy to help.",neoadmin,8
2015-12-02,http://blog.neospeech.com/2015/12/02/ultimate-text-to-speech-glossary/,The Ultimate Text-to-Speech Glossary,"Ever wondered what a TTS SDK could be? Or what TTS stands for? You’re not alone. Text-to-Speech technology is full of complicated phrases and industry acronyms and if you are new to this field, it can get a bit overwhelming. In this blog post we’re going back to the basics and looking at the definitions and uses of common speech technology terms. So if you’re ever a bit confused about the meaning of an acronym or word, you can come back to this post and discover what you need to know. For those of you who are just looking for an overview, we’ve put together this infographic as a summary of the all the key terms and definitions you need to know. And for those of you searching for a specific phrase or just looking to learn more, scroll down to see the complete and ultimate text-to-speech glossary. i 
 Let’s start with the general speech technology terms you’ll need to know and recognize, starting with the most general term, Speech Technology.  Speech technology refers to all technologies that aim to duplicate and respond to the human voice. This includes technologies that mirror the human voice, like text-to-speech, and technologies that aim to understand and process the human voice, like speech-to-text. Speech technology is the general field in which the study and development of text-to-speech falls into.  Text-to-Speech technology does exactly what it says – turns text into speech. We recently wrote an article specifically about   if you’d like to learn more.                   Voice Synthesis, Speech Synthesis  A common acronym for text-to-speech.  This is the core TTS technology that turns your text into speech. In basic terms, the text-to-speech engine takes your text, sorts it into linguistic segments (such as phrases and syllables) and assembles these into a large database. When you type text into the text-to-speech software, the engine analyzes your text and searches the database for the closest sounding speech units to your text, strings them together and produces them for you to hear.                   TTS Engine, Voice Synthesizer, Speech Synthesizer  Unit Selection Synthesis, or USS, is a text-to-speech method used to turn text into speech. While there are many different methods for turning text into speech, this is one of the most respected and commonly used methods. It is known for producing the most natural, human-like voices and preserves the original voice of the actor at all times.  A common acronym for Unit Selection Synthesis  HMM based Speech Synthesis, which often gets shorted to HTS, is another well-known speech synthesis technique. HTS uses a statistical model to generate the most similar sounding set of speech units to the text input. It produces a lower quality synthesized voice than USS. And, despite general belief, HTS requires more computing power than USS, which means it runs slower even with its small database. To learn more, check out this article about the  . 
  A common Acronym for HMM based Speech Synthesis System. HTS stands for “H and Three S’s” and refers to the first letter of each of the words in HMM based Speech Synthesis System.  The HMM in HMM based Speech Synthesis System stands for “Hidden Markov Model”  Speech recognition is the process of translating spoken words into text. An example is the voice activated assistant in your phone that you can instruct to call a friend or order a taxi. To learn more, LumenVox put together a  . 
  Speech-to-Text (STT), Automatic Speech Recognition (ASR), Computer Speech Recognition Linguistics plays a huge role in text-to-speech as develop new synthesized voices. Linguistics allows us to transform language and speech into something that a computer, namely the text-to-speech engine, can understand and utilize. Here in a guide to all the important linguistics terms used for TTS.  The scientific study of language and its structure.  Natural language is a human language, as opposed to a computer programming language. For example, Spanish and English are both natural languages, whereas HTML is not.  Often shortened to NLP, Natural Language Processing is a field of study that stretches across linguistics, computer science and artificial intelligence. NLP is primarily concerned with the interactions between computers and natural languages. This is critical in text-to-speech production as the TTS Engine must be able to understand and interpret natural languages.  A subtopic of NLP, which deals with machine reading comprehension. NLU enables computers to derive meaning from natural language input or from human interactions. This is the aspect of linguistics that allows the text-to-speech engine to understand the natural language inputs you have typed in.  A Phonetic Alphabet, also sometimes known as a Pronunciation Alphabet, is a set of symbols that represent the correct way to pronounce sounds in a certain spoken language.  Prosody refers to a collection of phonological features that are used to define the characteristics of a spoken language. Features include pitch, range, volume, rate and duration.  An audio processor that produces sound from an analysis of text input. It is essential in speech synthesis.  The linking of two items so that they can be treated as one thing. This next section relates to morphology, which is the area of study that enables us to examine the internal structure of words. Some of these definitions you will have heard before, such as syllables, but many of you might be new to the other definitions or even morphology itself. Note that we haven’t included definitions of morphology terms you definitely already know, such as “word”.  The study of the form and internal structure of words  A morpheme is the smallest possible unit of grammar in a language that still holds meaning. For example, the word “processed” is made up of 2 morphemes – “process” and “ed”.  Most of you will remember learning this early on in life, but just as a refresher, a syllable is a unit of pronunciation that has one vowel sound. This can be with or without consonants and together form part of, or sometime a whole word. For example, the word “synthesis” has 3 syllables – “syn” “the” and “sis”.  When discussing TTS software, a phone is no longer the cool mobile device that enables you to play games in your break time. Instead, a phone is a notation that represents a specific sound in a spoken language. Phones are usually letters, numbers or characters and are used to create the phonetic spellings to indicate how a word should be pronounced.  In phonetics, a diphone refers to an adjacent pair of phones. This term is most commonly used to refer to a recording of the transition between two phones. Text-to-speech technology is full of technical terms that can be overwhelming if you don’t have a computer science degree! Below you’ll find a guide to all the technical TTS terms you’ll ever need to understand.  The number of samples of audio carried per second. This is usually measured in Hz or kHz. This is used to describe the quality of the audio files produced by the text-to-speech engine. For example, NeoSpeech has 3 sampling rates you can choose from. 8kHz is best suited for IVR systems and emergency notifications, while the 16kHz and 44.1kHz sampling rates work best for other applications. 
  What a mouthful! While this acronym may seem like a lot, it is quite simple. TTS SDK stands for Text-to-Speech Software Development Kit. If you’re a computer scientist, you’ll know quite a bit about SDKs and if not, a TTS SDK is basically the tool kit that allows computer scientists to incorporate text-to-speech functionality into their application.  TTS API stands for Text-to-Speech application programming interface, which is a set of tools to aid in the building of TTS software applications.  SAPI stands for Speech application programming interface, which is an API that was developed my Microsoft to enable the use of speech synthesis and speech recognition on Windows applications.  Stands for Speech Synthesis Markup Language. Many of you may have heard of HTML, which is also a markup language and is used to write websites. SSML is an XML-based markup language specifically designed for speech synthesis applications.  This is NeoSpeech’s version of SSML, where VTML stands for VoiceText markup language. Any of our users can use VTML tags to edit the speed, volume, pitch and other aspects of the voice that pronounces the words typed into our text-to-speech engine. One of the most exciting things about text-to-speech software is the huge range of circumstances where it can be of use. TTS can be used in education, transportation, announcement systems, broadcasting, entertainment, finance and more. Let’s take a look at some of the more common uses of TTS and the definitions of those fields.  Computer Telephony Integration, or CTI, is a technology that enables a computer to interact with people. You may have encountered this technology when you call a customer service number. Here at NeoSpeech we have a lot of CTI and IVR customers.  This technology is a subset of CTI and allows the caller to respond by pressing keys on the keyboard or through voice response. IVR can also reroute a call to an appropriate employee if need be.  E-learning is any learning that takes place through the use of electronic technology, typically through the internet. These lessons usually take place outside of the classroom, but recent years have seen teachers start to integrate e-learning into their courses. E-learning that utilizes text-to-speech software is often centered around learning a new language or a course where it is beneficial to hear the words on the screen being said out loud.  These are technologies with aid people with disabilities. For example, text-to-speech can be used to read text out loud so that blind or visually impaired users of technology can consume content.  Often shorted to AI, artificial intelligence is the comprehension and intelligence displayed by machines or software. Text-to-speech is used to give a voice to robots and machines.  These kiosks provide access to necessary information in high traffic places, such as museums, stores, tourist attractions and shopping malls. This is where you would go to get information about how to get to a store with a mall or to find out the history of an artifact in a museum.  With the rise of e-books, you probably know what an audio book is. But just in case you don’t, an audio book allows you to listen to your favorite novel or textbook without having to read the words. To learn more about the different areas in which Text-to-Speech technology can be used, visit our   page.  If you’re interested in adding text-to-speech software to your application or would like to learn more about TTS, please fill out our   form and one of our friendly team members will be happy to help.",neoadmin,6
2015-12-21,http://blog.neospeech.com/2015/12/21/how-to-say-merry-xmas-in-5-languages-with-text-to-speech/,How to Say “Merry Christmas” in 5 Languages with the help of Text-to-Speech圣诞节快乐 (Shèng dàn jié kuài lè) ,"It’s that time of year. The wonderful time of year when we gather with friends and family, eat too much food, exchange some presents, have some fun and occasionally consume drinks with a larger than normal proportion of egg in them. At NeoSpeech, we’re passionate about learning and linguistics, so we’ve gathered our team and put together a guide to saying “Merry Christmas” in 5 different languages – English, Spanish, French, Chinese and Japanese. To help us out, we’ve asked our top quality text-to-speech voices to pronounce Merry Christmas in each of the languages so you can really impress your friends, colleagues or barista with your perfect pronunciation. For us, learning a new language is one of the best uses of text-to-speech technology, so let’s get into the holiday spirit and learn something new! i Christmas is celebrated widely throughout the English speaking world. It is a major holiday in large countries, such as the USA and the UK, as well as in smaller countries like New Zealand. Of course, Christmas is celebrated by Christians, but it is also widely celebrated by non-religious people who view the holiday as a nice reason to gather with the people they love. Families often put up a Christmas tree and decorate it with lights and hanging decorations and have a large family meal. Presents are then placed under the tree and opened on Christmas Day. This one is easy – if you’re reading this blog post, you can probably pronounce Merry Christmas just fine. But have you heard it in a British accent? How about an American one? Listen to Hugh’s smooth British English voice wish you a Merry Christmas or let one of our most popular American English voices, Julie, show you how it is done on this side of the world. Hugh – UK English   Julie – US English 
 i In Mexico, Christmas is celebrated for almost a whole month from December 12  to January 6 . Nativity scenes, knows as ‘nacimiento’ are very popular in Mexico. Sometimes the figures in the nativity scene are life size and a whole room in the house will be dedicated to the nacimiento. Christmas trees are not as common in Mexico as in the USA and the UK, with the nacimeineto being much more popular. Christmas is celebrated on the 25  of December, but Mexicans also widely celebrate El Dia de los Reyes (The Day of the Kings) where the 3 kings leave presents or candy on the 6  of January. While it is useful to see the phrase written on this page, we’re going to get Francisco, our male Mexican Spanish voice to pronounce the phrase out loud so you can hear exactly how to say it (and impress your friends and family). Francisco – Mexican Spanish   i Canada is a large country, the 2  largest in the world in fact, which makes it a melting pot of people from different cultures and countries. Because of this, there are many different ways in which Christmas is celebrated. Most families put up a Christmas tree, which is usually a fir or pine tree because the province of Nova Scotia is known world-wide for its pine and fir Christmas trees. Many Canadians open their presents on Christmas Eve and send Christmas cards to their friends and family. The Santa Claus Parade in Toronto is one of the oldest and largest of its kind in the world, having started in 1913. If you’ve never spoken in French before, the phrase Joyeux Noël could look at little daunting. Even if you took French in high school, it may be a long time since you’ve spoken in French. So, we’ve asked Chloe, our female Canadian French synthesized voice, to pronounce the phrase for us. Chloe – Canadian French   i In China, only   of people are Christians so the majority of the population knows very little about the religious side of Christmas. This means that Christmas is primarily celebrated in major cities, where Christmas has been quite commercialized as it is in many other countries. In these cities, there are large Christmas Trees and stores are decorated with festive lights and decorations. Here, Santa Claus is referred to as ‘Shen Dan Lao Red’, which means Old Christmas Man. Most people only see a Christmas tree in shopping malls and very few people will have one in their homes. Technically, the phrase Shèng dàn jié kuài lè translates to Christmas Happy, but it is the phrase used to say Merry Christmas in China. If you’re a native English speaker, this might be a hard one to pronounce, so sit back and listen to Liang, one of our male Chinese synthesized voices, say this festive phrase. Liang – Mandarin Chinese   i Like China, there are very few Christians in Japan meaning that Christmas is not widely seen as a religious holiday. Christmas is not a national holiday in Japan, meaning that businesses and schools are still open. Christmas has only recently become a celebrated event in the last few decades and is seen as a time to spread happiness rather than celebrate a religion. The Japanese people have recently starting taking on more USA and UK based traditions, such as the sending and receiving of gifts at Christmas time. Christmas Eve is viewed as a romantic day, where young couples exchanges gifts and go out for dinner; the day is similarly viewed in Japan as Valentine’s Day is in the USA. In terms of Christmas food, fried Chicken is the popular food of choice, with Christmas being the busiest time of year for local KFC restaurants. Like Mandarin Chinese, the Japanese phrase for Merry Christmas, Merīkurisumasu, can be rather hard to pronounce if English is your first language. Here is a clip of one of NeoSpeech’s female Japanese voices, Misaki, spelling out the phrase so you can bring a new element of culture to Christmas this year. Misaki – Japanese i How do you celebrate Christmas? Did our text-to-speech voices help you learn something new? Let us know in the comments below or email us at  . 
 NeoSpeech’s text-to-speech technology is useful for all kinds of learning, including learning a new language as we did today or taking an e-learning course online. We have a   with 30 voices and 7 languages, which you can find out more about by viewing our products page. Alternatively, if you have any questions or you’d like to talk to someone about adding text-to-speech to your next product or e-learning platform, please fill out our   form and one of our friendly sales team members will be happy to help. Or, if you’d like to learn more about Christmas traditions around the world, take a look this great resource,  . 
 i",neoadmin,9
2015-12-29,http://blog.neospeech.com/2015/12/29/how-4-influential-linguists-changed-history/,How 4 Influential Linguists Changed the Course of History(4th century B.C.E.)(1767 to 1835) (1857 to 1913)(1928 to present),"If you take a moment to think about, from the time of your birth, in which communication was reduced to only a number of audible sounds, to now, your language has been shaped by a vast sea of knowledge consisting of making meaning out of those, now more sophisticated, sounds. Of course, every person who has ever had a fight with their spouse can tell you that meaning would never come down to just literal noise because when, how, and where we say our words truly affect how our audiences receive them. That example is only a snippet of what many mechanisms drive our languages. Behind our words are an endless collection of symbols, signs, and patterns that have evolved over a long period of time. Even a gap as small as a single generation can illustrate just how much our languages can change (refer to a recent conversation you’ve maybe had trying to explain something to a parent; it’s not as easy is it?). This begs the question as to how such a phenomenon could even take place at all; is it by a mere coincidence that these grammars and rules were born or are there phenomena that drive our languages to take the form they do today? The field of Linguistics exists in order to help us uncover the answers to some of these questions; to make sense of the brilliant, yet potentially chaotic noise that surrounds us every day. It concerns itself with the scientific analysis of morphology (structure), syntax (arrangement), phonetics in terms of vastly improving understanding, communication, preservation (in some cases), and education. Whenever you think about philosophy, the names of Kant or Aristotle inevitably come to mind because they have, for the better part, defined an entire school of thought. On the other hand, when teachers introduce the sciences, pioneers such as Sir Isaac Newton or Albert Einstein make their way across the chalkboard because they’ve changed our perception of reality. The same applies to linguistics, in that there are a few names that you just have to know because their discoveries and contributions were so great. It is because of individuals such as these, that our understanding of language has been redefined and the foundations were set for others to build upon. Let’s meet a few of these inspirational linguists: h Often referred to as The Father of Linguistics, Pāṇini ’s work made an incredible impact on the study and preservation of languages. Similar to the likes of Socrates, Homer, or Confucius, there are very little accounts that exist that can actually pinpoint the exact timeline of Pāṇini’s life. But what he is universally credited for, however, is being architect to one of the world’s first formal language structures. Known as  in Ancient India, his detailed algorithmic system took in the countless spoken sounds and symbols that the people of his region used at that time and, through the mixing/matching of many “stems” and “roots,” generated banks of well-formed words with little to no redundancy. His  compiled of 8 chapters with nearly 4,000 rules outlined, was the first form of linguistic analysis that scholars can refer to. Pāṇini s work aimed to bring order to the otherwise chaotic state of languages in his time. The initial purpose of his work was to preserve knowledge of the language of the Hindu religious canon. This language, along with many other languages in his time, changed so frequently that recitation and understanding of important religious works could not be assured throughout the population, even though people claimed to speak the same language. Pāṇini  was one of the first linguists to develop language and grammar structures to preserve a language. The basis of these grammar rules was to enable two parties to settle a dispute and to begin to work together to improve their common language. These formal language structures also enabled religious works to be understood by everyone who spoke a certain language, achieving Pāṇini’s original goal. This concept may sound simple – the idea of basic grammar rules that everyone using a certain language abides by – but at the time, this was a revolutionary idea and laid the foundations for the study of linguistics. As we’ll see, Pāṇini’s work would serve as a springboard to a lot of modern linguistics as we know it today. Many modern linguists have improved upon and developed theories by understanding the works of Panini, allowing them to develop new linguistics techniques and tests to better understand our use of grammar and language as a whole. h Philosopher and diplomat for the kingdom formerly known as Prussia, you’d probably recognize Humboldt first as the man behind the concept of   before you would a linguist. Nevertheless, his contributions to the field were instrumental at a time in which we needed to make further sense of language. Up until his time, philosophy and empirical linguistics were always thought to be two separate things; many felt that language, despite its complexity, was just a mere collection of markers and symbols that give things meaning. Throughout his travels as a political diplomat, Humboldt began to challenge such a notion after several of his works included German translations of various foreign classics, as well as extensive examinations of the Native American grammers and language that were brought back from the Americas. Humboldt argued that if language was indeed independent, then possessing linguistic capacity and competence should all but guarantee the understanding of one another; otherwise suggesting that meaning is either inherent within the symbols themselves or even biologically speaking. However, by simply examining our own lives, we know that none of us are born predetermined to speak English for example, rather it just so happens to be the context in which we are birthed and raised in. Off this logic, Humboldt began to uncover a side of linguistics that had been previously overlooked, in that he saw language as a representation of the world, and not just a mixture of finished sounds and signs. In his famous paper entitled   Humboldt began to outline how modern linguists would interpret and study language for generations to come. He was the first to mesh the lines between philosophy and linguistics, demonstrating that language exists fluidly within the world of thought, and not because of it. He articulated that humans, in an never-ending quest to make sense of phenomena in front of them, will drive and create language themselves. In a sense, you can label Humboldt’s line of reasoning to be almost anthropological, in that it is very much the study of humanity before it is grammar or markers. When correlating it over to a language, he compared it to the studying of a dead skeleton or fossil, in that archaeologists aren’t necessarily concerned with the material bone itself, but rather with the identity, what the being was able to achieve, and perhaps any insight into the ideology or social space of that time. We know simply by observation that language has and will change over time; that was never a secret. But similarly to the fields of science or mathematics in which we can look at in retrospect, it took the work of a select few to articulate and shape how humanity would move forward in that line of thinking. h This Swiss academic, depending on who you ask, is often referred to as the “father of modern linguistics.” Saussure was the first to champion the idea of  , which postulated that the meaning derived from language is done so not because there are objective truths to each object, but because the entire body as a whole is self-referential. Now, we understand that’s a mouthful to process, but ask yourself why you call a cat, well, a “cat.” When you think about it, you start to see that there is actually no natural reason other than what we can refer to in our language, that makes “cat” the name we choose for our feline friends. Oddly enough, Saussure’s greatest work wasn’t published until  his death in 1913. Compiling his notes and material from his time studying Indo-European languages at the University of Geneva, two of his students helped assemble one of most influential books on modern linguistics titled,   introduced Saussure’s innovative approach in tackling language. At its core, similarly to his predecessors, Saussure believed that we could no longer study languages as these separate phenomenons; rather he saw that they all have some formal logic that link them. Theoretically, his work began inferring that all of our languages could be traced back to deviations of Proto-Indo-European sounds and vowels. So how does this all apply to the “cat?” Well, Saussure deduced that our understanding of the word itself arises from nothing organic; we didn’t emerge from the womb intuitively knowing words and their definitions. What he found was that we as individuals take words and compare/contrast them to others within our “bank” as you will. Meaning when I hear “cat,” I contrast it to words such as “mat” or “car,” and then compare it to equivalents such as “kitty,” “feline,” or “kitten.” This line of logic would shape the thinking of many linguists moving forward, as the understanding what language is in a given “snapshot” of time becomes increasingly more important when hoping to decipher meaning through similar self-reference. h Perhaps the most polarizing of them all is Noam Chomsky (featured on the left side in the photo), an American linguist who has pushed the envelope further than any of his predecessors have before. Both celebrated and often even challenged throughout his career that has stretched well over half a century,    on the idea of  transformational and universal grammar have single-handedly revolutionized how languages are studied today. Before him, linguists were already starting to consider the idea that languages were somehow deeply connected to one another; but what Chomsky proposed was even braver than anyone would have imagined. For starters, his theory completely went against conventional thought (at that time), by declaring that language itself was indeed  just a means of communication; rather it was a mode of  itself. He formulated that linguistic structures are in actuality genetically “pre-programed” in humans regardless of socio-cultural differences. Meaning we would have to look at the human just as much as we would the characters of a text. His studies believed that teaching was actually not essential to the acquisition of language at all. Chomsky proposed that our innovative brains were actually the drivers in formulating language, and that any other formal structures or rules are only ramifications of a social and cultural context. Based on this argument, he observed that any person, a baby even, would have the capacity to reason with their thoughts, and eventually (after much time of course) refine them into an appropriate medium. Furthermore, that premise would come to shape what Chomsky championed as  a linguistic approach that explains the ability of a hearer-speaker to produce and interpret an infinite number of utterances. In melting the lines of social and geographical even further, Professor Chomsky’s idea of universal language would culminate to the introduction of the   which partitions formal grammers into various classes or types. In his model, each successive class meant a morphing or added complexity to that of its predecessor which in theory can continue branching out with the passing of time. Linguists can use this to better identify common themes, patterns, and links that tie virtually  human language together under one umbrella, and understand how languages may or may not naturally change over time and distance. Outside of linguistics, but also metaphysically tied to it, Professor Chomsky continues to be very involved in political activism. His name comes up as one of the more outspoken voices when it comes to American policy, and he has made several rounds throughout the years speaking on capitalism, war, and authority. He is regarded as one of the most important thinkers of our time, and continues to hold his post as Institute Professor Emeritus at the Massachusetts Institute of Technology (MIT) at the spry age of 87. h Even if it is just through these four individuals, it’s fascinating to see how far our understanding of language has come over the last 2000 years. On one end of the spectrum, the original work done by Pāṇini sparked an awakening in terms of defining and organizing the medium for the first time. Those that followed him such as Humboldt, Saussure, and Chomsky, inspired by the empirical approach, launched their own explorations into what languages really mean, and how they are derived. They, like the pantheons of other parallel sciences, made sense of the realities of our world. What we thought was just a collection of quantifiable things we could observe turned out to be much more. Language, and our understanding of it, is constantly evolving over time. Linguists are those who bring it to life. Those who are preservationists study and protect endangered languages; educators teach language whether it be a foreign one or an ESL class; and computational linguists are those who adapt language to artificial intelligence, speech synthesis, and speech recognition nowadays. Throughout its many shapes and forms, the people who drive it are the linguists. Next time you hear another language on the street, or share a conversation with Siri, think about the amazing minds behind it all. h",neoadmin,3
2016-01-18,http://blog.neospeech.com/2016/01/18/bbc-uses-text-to-speech-to-translate-broadcasts-into-japanese/,The BBC takes on Text-to-Speech Technology to Translate Broadcasts into Japanese,"The British Broadcasting Corporation (BBC) is trialing a new innovative ‘virtual voice-over technology’, which uses a combination of technologies to efficiently and precisely translate short online news videos into languages other than English for the world to enjoy. The technology is an amalgamation of other existing technologies, pulling together text-to-speech software and automatic translation technology to create a unique virtual voice-over application. The use of this technology streamlines the process of translating English videos into other languages. When a journalist produces a broadcast, they can utilize this new ‘virtual voice-over technology’ to translate their entire broadcast into another language in a matter of minutes. The process is simple. First, a journalist will upload the written script of their broadcast to the system, which will automatically translate the English text into another language. Each journalist then has an opportunity to proof-read and edit the text translation to ensure it is correct. Once the content is confirmed, the computer program will generate an audio file of the broadcast using a synthesized voice. As the technology is improved, the ability for automatic subtitles will also be included. This is an exciting new use of text-to-speech technology, enabling people throughout the world to gain access to some of the best journalism in the world, regardless of what language they speak. Here at NeoSpeech, we firmly believe that language barriers should never hinder the learning experience and the pursuit of knowledge. The technology first went live in December 2015 with the integration of a range of Japanese TTS voices, including our very own Show. The BBC plans to integrate Russian synthesized voices into the trial platform later in 2016 and if the trial is a success, they will look into expanding the program into even more languages. Not only does this increase access to current affairs, but it also increases the efficiency of journalists at the BBC, enabling them to focus on discovering and reporting top stories and focusing less on the translation of those stories. To learn more,   where our UK English voice, Hugh, guides a demonstration of the BBC’s exciting new technology while the BBC’s technology experts discuss their future plans for the new technology. “I’m very excited about this trial” says James Montgomery, the Digital Director for BBC News. “The BBC has some of the best original journalism in the world, with correspondents around the globe. Technology like this means we can bring more of our international journalism to more people.” We are excited to see where this technology takes the BBC and to see our text-to-speech software integrated into other ambitious projects like this throughout 2016. This is an important step in enabling the world to gain access to current affairs and to keep everyone up to date, no matter what language they speak. o To learn more about the BBC’s new technology and use of text-to-speech voices, take a look at these articles from the BBC: Considering adding text-to-speech to your next application? Take a look at our   or   and talk to them about creating a customized solution for you. o",neoadmin,8
2016-01-04,http://blog.neospeech.com/2016/01/04/quizlet-neospeech-partner-raises-12-million-series-funding/,"Quizlet, a NeoSpeech Partner, raises $12 million in Series A funding",", one the world’s most popular online learning tools, has raised $12 million in their first round of funding. Quizlet has long been a partner of ours, so we wanted to share their success with you and what it means for the future of online learning and text-to-speech use throughout the world. Quizlet uses our text-to-speech voices to fuel their online learning platform, enabling users to hear their study material or tests out loud. People who are learning a new language can use Quizlet to study and learn, utilizing one of NeoSpeech’s synthesized voices to learn exactly how to pronounce a word or phrase without the need for a tutor. Quizlet’s learning power extends well beyond just learning a new language. Students can create their own study materials based on what they want to learn, meaning that they can learn virtually anything from mathematics to geography. Using text-to-speech, students can learn visually and aurally, which often results in a more comprehensive understanding and heightened recall and retention as they can use more senses to process information. To add to this, the addition of text-to-speech software to Quizlet enables users with visual disabilities to take part in Quizlet, opening up e-learning to those with accessibility needs. i j Quizlet was originally founded and developed by Andrew Sutherland in 2005 as a study tool for French class. After 10 years of hard work, Quizlet ranks in the Top 50 most visited websites in the USA and is used by almost 25% of all high school students in the USA. Quizlet is determined not to stop there – the $12 million they raised will be used to expand Quizlet internationally to reach 1 billion students worldwide. We believe that this is the future of learning – a worldwide platform where users can interactively study any material they wish using multiple senses at a time. NeoSpeech is excited to see Quizlet’s growth and support them as they move further into the international market by providing the best text-to-speech voices available. i To learn more about Quizlet, visit their website  . To learn more about Quizlet’s fundraising, check out   from Andrew Sutherland or this article from the  . 
 If you’d like to learn more about Text-to-Speech, check out our   for embedded, IVR, e-learning and personal use. Alternatively, you can fill out our   form and one of our friendly sales team members will help answer any questions you have or help you add text-to-speech software to your next application. j",neoadmin,6
2016-02-18,http://blog.neospeech.com/2016/02/18/medical-schools-urgently-texttospeech/,Why U.S. Medical Schools Urgently Need Text-to-SpeechStudy: Most U.S. Medical Schools Do Not Accommodate Students with DisabilitiesWhy Is It Important To Accommodate Students with Disabilities?Applications of TTS in Medical Schools,"Text-to-Speech software is currently revolutionizing the medical industry (See #4 on our blog about the  ). Over the past few decades, specialized devices have been made that allow doctors to better perform patient care, such as heart rate monitors and dialysis machines. But in the past decade, TTS has given these devices a voice, which has greatly improved their usefulness. This is especially the case for someone with a disability. For example, a blind person has the power to check their own blood glucose level at home, all thanks to a blood glucose meter with built in TTS capabilities that allow the device to recite the results to the user. Devices today are utilizing TTS technology. Now, it has become clear that TTS can greatly benefit the health care industry in another way. Medical schools are now being pushed to better accommodate students with disabilities, and making sure they get the education they need. This is where TTS comes in. A   has concluded that most U.S. medical schools are not doing enough to support students with disabilities as intended by the Americans with Disabilities Act. The infographic below shows some of the results of the study: Medical schools are legally required to provide reasonable accommodations, and the researchers concluded that not enough is currently being done. In addition to the obvious answer that all students with disabilities should have accommodations that provide an equal opportunity, there is actually a need in the U.S. for more health care professionals with disabilities. Studies have shown that patient care outcomes are improved when there’s more representation and diversity among health care professionals. People also tend to like doctors who have similar disabilities as they do. An example of this would be a person with a hearing disability being treated by a doctor with a similar disability and who has endured similar experiences in life. 20% of the American population has a disability, while 3.2% of Americans aged 18-24 have a disability. However, less than 1% of medical students have a disability. This underrepresentation of people with disabilities is a problem because as stated earlier, diversity among health care professionals can improve patient care outcomes. We already discussed how TTS is revolutionizing the medical industry, but can it have the same impact on medical schools? The answer is yes. It happens in a similar way that medical devices use TTS technology to help people with disabilities. TTS technology can help improve the learning experience of students with disabilities. Multiple customers of NeoSpeech use our products and service to  . This has given students with disabilities access to information that would have been difficult for them to obtain before. Examples include E-readers that have made it possible for students to hear any and all information a book has to offer. This can be done with an electronic book or even an application that scans text and reads it to the user. TTS technology has also been used for training simulations, such as an application or device that virtually trains a student on open heart surgery! Do you agree that there is a need for more representation of people with disabilities among health care professionals? Do you have any experience with TTS in an eLearning or medical capacity and do you have any thoughts on it? Let us know in the comments or send us an email at  . To learn more about the different areas in which Text-to-Speech technology can be used, visit our   page. And to learn more about the products we offer, visit our   page. If you’re interested in adding Text-to-Speech software to your application or would like to learn more about TTS, please fill out our   form and one of our friendly team members will be happy to help.",neoadmin,9
2016-01-22,http://blog.neospeech.com/2016/01/22/top-5-speech-technology-events-in-2016/,"Top 5 Speech Technology Events You Should Attend in 2016Best Speech Technology Event Overall: Top CTI and IVR Event: Assistive Technology: 
E-learning and Education: Broadcasting: ","2016 promises to be a big year for Speech Technology.   and   are expected to make giant leaps forward. Speech recognition technology continues to improve, with   in 2015. Facebook is working on text-to-speech technology to  . Here at NeoSpeech, we plan to create numerous new high quality voices and work with new clients to implement text-to-speech in ways that have not been seen before. With these incredible advancements in mind, 2016 is sure to be an exciting year in the speech technology space. As the creators of some of the world’s best text-to-speech software, we carefully consider which events and conferences to attend, ensuring that we harness every opportunity to talk with new potential customers and foster relationships with current ones. We have a long track record attending and exhibiting at many of the world’s top speech technology events, so we know a great event when we see one. So, where do you need to be to be a part of this innovation and excitement? How can you meet the best new customers and discover new technologies and strategies applicable to your field? Take a look at the Top 5 Speech Technology related events to attend in 2016 below. Note – As you may know, speech technology providers and customers are prevalent in a variety of fields. Hence, this list extends far beyond just speech technology events, presenting the best events to attend if you or your customers are interested in a variety of related fields, such as education, accessible products, CTI, IVR and more. i   SpeechTek is THE event to attend if you’re interested in or working within the speech technology space. This is the opportunity for speech technology companies to learn about new improvements and advancements in the field, how to apply those to their business or situation, and to meet and learn from experts.   SpeechTek is the ideal event for people from businesses in all areas of speech technology. Everyone from top level managers to your newest salesperson can gain a lot from attending this event, including learning about your competition, discovering new innovations in your field and how you could implement them in your services and products, and meeting potential new clients. SpeechTek also has numerous benefits for people outside the obvious areas of speech technology. SpeechTek is a great place for businesspeople from fields such as IVR and CTI, e-learning, accessible devices and mobile app developers to seek out and find new business partners and learn how to integrate speech technology into their products and services. Customer Services Managers, Consumer Electronic Designers, Customer Experience Designers and other members of the customer service industry can easily discover new ways to make their customer experiences more effective and efficient using speech technology.   SpeechTek facilitates learning about new and improved speech technologies and the many applications of speech technology. Attendees will learn how to improve customer satisfaction and loyalty using speech technology and discover most innovative principles in voice interaction design. There are a fantastic range of keynote presentations, panel discussion, workshops and opportunities to network. The best and brightest of the industry always attend this event making it the perfect place to meet a new business partner or to size up the competition. SpeechTek is a single, comprehensive source for information about all kinds of speech technology – so don’t miss it.  Omni Shoreham, Washington D.C.  May 23-25, 2016. :  Get $100 off Early Bird Registration or a free Expo pass by using the code   when registering! Register here:  i   Call Center Week is one of the largest customer care conferences in the United States. The conference aims to educate businesses about meeting the demands of customers, understanding and utilizing data, leveraging cloud computing opportunities and how to prove ROI in a call center.   If you’re in the Call Center industry or interested in improving your customer service, this event is a must. Everyone from middle managers to C-level managers will learn new ways to improve and optimize their customer service by attending this conference and hearing from customer service experts. This conference is also a great opportunity for those in the speech technology industry to meet potential new customers to enable them to leverage speech technology and improve the efficiency and effectiveness of their call center and customer service.   This year will be the 17  Annual Call Center Week, highlighting the longevity of this event’s usefulness and attractiveness for those in the call center and customer service industries. You will hear from and meet leading minds in the customer service industry, learning all about how to implement the latest customer service strategies in a range of industries. Experts from throughout the world travel to attend this event, providing opportunities for you to discuss international customer service strategies with those around you. According to CCW, 58% of their attendees have at least 5 years experience in strategic customer service. It is the perfect event for call center professionals and customer service experts to network and learn. To add to this, people from a huge range of other industries attend the event, including those with a speech technology background. This provides a fantastic opportunity to network and heard about unique implementations of customer service strategies. Speech technologies, especially text-to-speech technology, are used in a range of IVR and CTI applications. From small business who want pre-recorded audio for their answering machine to make it sound more professional to large scale call centers who want to enable customer to check their bank balance or book a flight without the need to speak with a representative. If you’re in the speech technology field, this is the perfect opportunity to meet small and large scale CTI and IVR customers and to promote your products to them.  The Mirage, Las Vegas, Nevada  June 27 – July 1, 2016 :  i The Assistive Technology Industry Association (ATiA) Conference claims the title of the largest international conference that showcases advancements and innovations in assistive technology. For those of you who do not know, assistive technology refers to any technology that assists people with disabilities, such as screen readers and text-to-speech apps. The ATiA Conference offers an opportunity for anyone who works in the assistive technology field to network and learn from the best minds in the industry.   This conference is catered towards those who are interested in discovering and/or presenting the latest advancements in assistive technology. While anyone can attend, the conference is designed for AT practitioners, teachers, OTs, PTs, or any other professionals who find value in the networking opportunities provided by the ATiA conference. ATiA is also a great event for speech technology professionals to attend, providing opportunities to discover new ways in which your speech technology could be used to aid those with disabilities or to meet new potential customers.   ATIA is the best place to learn about advances in the assistive technology field and discover how to integrate these new innovations into your workplace, products or services. Like all our other events so far, this is the perfect place to network as it attracts all the best minds from throughout the industry. The sessions look fantastic this year too, with overview sessions such as   and more specific sessions for targeted attendees, such as  . There is definitely something here for everyone. One of the reasons we work in the text-to-speech space is to aid people with disabilities. We believe that everyone deserves access to all emerging technologies. The ATiA Conference is the perfect place to meet people who design and distribute accessible products, many of whom are searching for speech technology to incorporate into their products. These are passionate potential customers that you won’t meet if you do not attend the ATiA conference.  Caribe Royale All-Suites Resort & Convention Center, Orlando, Florida  February 2-3, 2016 :  
   DevLearn is one of the biggest and best learning technology events in the USA. From online e-learning courses to implementing new technologies in the classroom, DevLearn invites everyone who is interested in the intersection of learning and technology to attend.   Principals, teachers, e-learning business professionals, HR managers and anyone who is passionate about learning and utilizing innovative learning technologies are obviously encouraged to attend. However, DevLearn opens the door to businesspeople in a range of industries, as professional development and learning happens at all stages of life and in all fields of expertise. Businesses in the speech technology space, like us, should attend to meet new B2B partners and increase brand exposure, as e-learning is one of the biggest applications of text-to-speech technology in recent times. DevLearn provides a comprehensive guide to learning technologies in 2016. It is always hosted near the end of the year (mid-November in 2016), enabling the organizers to present the information and strategies that made the biggest impact throughout the year. DevLearn attracts the most qualified learning and e-learning speakers, with last year featuring Adam Savage from Mythbusters, Connie Yowell, the Director of Education at the MacArthur Foundation and other inspirational minds in this field. You can attend presentations about learning technology strategies, discover new applicable technologies, explore how to implement new strategies and technology and network with other people with similar roles or in similar fields to your own. The field of e-learning is one of the biggest and most versatile providers of text-to-speech customers. At NeoSpeech, one of our specialties is working with e-learning products, making DevLearn a must. We work with everyone from e-learning providers creating online language learning tools to HR departments looking to implement new company training systems. Many of these customers attend DevLearn – and if you want to meet them, you should too.  The MGM Grand in Las Vegas, Nevada  November 16-18, 2016 : The ELearning Guild has not yet released their official DevLearn 2016 website, but you can get some general information about it here:  i   The NAB Show, produced by the National Association of Broadcasters, is the largest video, audio and broadcasting event in the world. This is where broadcasting professionals come to learn from and network with the best.   The NAB Show is   event for radio and television broadcasters and broadcasting businesspeople. The NAB Show also attracts many professionals in the digital media industry as well, such as producers, directors, C-level executives and other experts with a passion for producing content across visual platforms. Anyone with an interest in producing and promoting content on television, radio, the internet, and movie theatres will find this event engaging and interesting. As I said before, the NAB Show is the world’s largest event for broadcasters’ and digital content creators; it’s so large it is the only show on this list with  . The NAB Show offers anyone in the realm of broadcasting the opportunity to find new technologies, news ways of implementing current technology, and new talent. There are many fantastic sessions you can attend from high profile speakers on topics such as feature film making, radio broadcasting, new media platforms and more. There are over 100,000 attendees every year from 166 countries, showing that there are a ton of people who believe this event is worth it. The event has been running for over 85 years, consistently attracting the experts and innovators in the field. The NAB Show has a special attraction for speech technology enthusiasts too. The NAB Show is the perfect place to meet potential customers and find strategic partnership opportunities. In light of the , broadcasters are now more aware than ever of the benefits of using text-to-speech. These new rules mean that television shows that are broadcasted with text-based emergency information that is not read aloud on the main screen need to provide a secondary audio stream dedicated to conveying this emergency information. There may be unique opportunities at the NAB show to educate broadcasters even further and open up secondary uses of text-to-speech technology to help grow your business.  Las Vegas Convention Center, Las Vegas, Nevada  April 16-21, 2016 :  i Attending speech technology related events is a key driver of growth and customer acquisition in the speech technology arena. Whether your business focuses on delivering text-to-speech to broadcasters or speech recognition software to IVR producers, there is an event here that is definitely worth attending. Even if you decide not to exhibit at these events, you can still obtain great value from attending. i With all these incredible events this year, 2016 is sure to be a success. Take these opportunities to meet thought leaders, experts and like-minded people in your industry, discover the latest breakthroughs in recent technologies and learn how to implement internationally renowned strategies. While this sounds like a lot of marketing hype, this is actually what you will achieve by attending these conferences – you can take our word for it. No matter what area of speech technology, education, technology or another industry you’re in, there is something here for everyone. We’d love to hear about your experiences at these conferences or what you’re looking forward to most about these events in 2016. Feel free to comment below or email us at   i One thing we love about events and conferences in the ability to meet new clients and continue to solidify relationships with current partners. If you’re considering adding text-to-speech to your next product, application or service, don’t hesitate to reach out to us through our   or take a look at our  .",neoadmin,8
2015-09-17,http://blog.neospeech.com/2015/09/17/neospeechs-free-text-to-speech-apps/,The Line Up: NeoSpeech’s Free Text-to-Speech AppsSimple User DesignAbility to Save Your Text and Audio FilesCopy and Paste Text into the App5 of NeoSpeech’s Synthesized Voices to Choose FromDesigned for Commuters and News EnthusiastsCustomize Your CategoriesOther Cool FeaturesDo you use any of our Apps? What do you think?Creating Your Own App with Text-to-Speech?,"In light of the release of Spokesperson, our most recent text-to-speech (TTS) app, I thought we’d take a look at NeoSpeech’s Apps and how they can be used. While we think our apps are great, the primary purpose of this post is to shows what each app offers and how you can benefit from using them. Whether you want an app to read the latest news out loud to you on your way home or to aid in communication, we have apps to help you achieve your goals. As I mentioned, Spokesperson is our brand new text-to-speech app, which was only released in the App Store a week ago! (You read more about that  ). 
 In basic terms, Spokesperson enables you to type in any text you like, choose one of our synthesized voices, and let our text-to-speech engine turn your text into speech. Nice and simple! Spokesperson is primarily designed to enhance accessibility for visually impaired or mute smart phone users. Basically, it is our solution to breaking down some of the communication barriers that people with disabilities often face. When we sat down to design Spokesperson, we agreed that we wanted to design a portable text-to-speech engine that people could access anywhere, anytime. A lot of text-to-speech products are big or confined to desktop computers, so we wanted to enhance the accessibility of TTS for everyone. And we figured that the best way to do this was to create a free app so that everyone can utilize TTS technology wherever they go – and no one has to carry a computer around! We want this app to have a positive influence on the lives of people with disabilities, which is one of the reasons we made it free. So, check it out! And if you’re not yet convinced, here’s a bit more information about the app. The app is designed for multiple uses every day, so we wanted the interface to be as easy to use as possible. By keeping the design simple, you can easily open the app, type in the text you want spoken and let the TTS engine do the word without confusion. We wanted to enable users to save their previously developed text and audio files for later use. This means that when you create your own introduction file, such as “Hi, I’m Sarah. It’s nice to meet you” and then you can save this file for use anytime. So, when you meet a new person, you can simply bring out the Spokesperson app, click on this file, and NeoSpeech’s text-to-speech engine will automatically turn the text into speech. No need to type out that message every single time. Multiple files can be saved to enable communication to be as simple and effective as possible. Spokesperson enables you to copy and paste text into the app and hear it read out loud. This is useful in a range of situations, such as reading text messages out loud or clarifying what a paragraph says. We wanted to give you the freedom to choose your voice, so all our US English voices are included in the app for free! There are 3 female voice options and 2 male voice options, including: Looking to get up to date on the latest gossip, trends or news?   – it utilizes our text-to-speech technology to read all the latest news out loud to make keeping up to date enjoyable and easy. NewsSpeak is primarily designed for commuters; for people who want to give their eyes a break from staring at bright screens. There are a lot of people who like to sit back, close their eyes and get up to date while on the train or bus using this app. Personally, I use the app while I’m driving. I just open it up and pick the article I want to hear (before I start driving of course!) and then listen to the news on my way to and from work. It is the best way to stay safe and up to date. However, the app is not only for commuters. It can also be useful for relaxing at home and listening to the news or for people who are visually impaired. It is great for all news enthusiasts! NewsSpeak has a great range of categories for you to choose from. Whether you want to catch up on the latest celebrity gossip in the Entertainment section or hear who won the game in the Sports section, there is news for everyone. Categories include: One of the best things about this app is the ability to customize which news you want to hear and see. You can easily choose which of these categories you want to consume news about and NewsSpeak will only show you those. The NeoSpeech Text-to-Speech LITE series includes 4 apps which turn text to speech using our high quality synthesized voices. This series of apps have the same user interface but differ depending on what language you’d like to use. You can choose from US English, UK English, Canadian French and Mexican Spanish. The key difference between these apps and the Spokesperson App is that these use embedded Text-to-Speech whereas Spokesperson uses Server Text-to-Speech. This means that when you download an app in the LITE series, you are also downloading the TTS engine to your device. The main advantage to this is that you can use the TTS functionality without connecting to the internet. But there is a trade off – the size of the app is much larger than the Spokesperson app because you have to download the whole TTS engine. The Spokesperson app, on the other hand, uses a cloud-based server for TTS. The file size is much smaller but you must be connected to the internet for the TTS to work. So, it depends on what kind of flexibility you want. Also note that the LITE series only allow up to 15 words to be read at one time. Because of this, I’d recommend checking out the Spokesperson app first as you can have a higher word limit to play with. But these apps are great if you’re looking for a free, portable TTS app, especially if you’re looking for mobile TTS in a language other than US English. Download Here: Download Here: Download Here: Download Here:",neoadmin,8
2015-09-28,http://blog.neospeech.com/2015/09/28/introducing-fccs-accessibility-regulations/,Introducing the FCC’s Accessibility Regulations,"The Federal Communications Commission (FCC) is implementing regulations that mandate that emerging technologies are accessible to users with disabilities. This affects a range of businesses, from broadcasters to TV manufacturers to smart phone producers. These businesses need to make sure that people with disabilities can use their product or service. But what exactly are these regulations? Will they affect your business? And what do you need to do to prepare for this change? In this blog post series, we will explore the fundamentals of the FCC’s 21  Century Communications and Video Accessibility Act (CVAA), its regulations and how they affect a range of businesses. This blog post will introduce you to the CVAA, an act you will become quite familiar with by the end of this blog series. The following blog posts will dive into more details about the 2 titles of the act before discussing what this means for businesses and how you need to prepare. In 2010, Congress passed the   to ensure that new forms of communication and video are accessible to users with disabilities. This act follows a series of laws that were passed in the 1980s and 1990s to ensure that all telephone and television services were accessible to Americans with disabilities. However, these laws had not since been updated to account for the growing number of new technological devices and changes that we have witnessed since then. The daily use of mobile phones, tablets, laptops and other emerging technologies was not accounted for in the technology accessibility laws of the 20  century. So, the 21  Century Communications and Video Accessibility Act was created containing new regulations to enable people with disabilities to utilize emerging technologies of the 21  century. The CVAA is rather long and somewhat dry, so we are going to break it down and present an overview the main titles and what this means for businesses. However, if you would like to learn more about the details of the act, every blog post in this series will finish with a   section where you can find additional resources to help you in your research. Basically, the CVAA is broken down into  . Title 1 aims to make products and services that use broadband fully accessible to people with disabilities. This means that any device that connects to the internet must be able to be used by people with disabilities. This includes smart phones, laptops, tablet, smart TVs, VoIP providers, gaming consoles and more. This title also mandates that these devices enable Text-to-911 functionality, so that all users with disabilities can contact and communicate with emergency services. Title 2 aims to make it easier for people with disabilities to view video programming on television and on the internet. For example, shows that are displayed on television and then uploaded to the internet will be required to have captioning when they are re-shown on the internet. The other main part of Title 2 requires the use of a secondary audio stream to be dedicated to the aural presentation of any emergency information that is presented in a purely visual format during a show. It is important to understand what disabilities the FCC is referring to when they define that your product or service must be able to be used by people with disabilities. The CVAA is designed to enable people with the following disabilities to utilize emerging forms of technology, such a tablets, smart phone and smart TVs. If you’d like to learn more about the CVAA, here short is a list of valuable resources: h       h The CVAA is broken down into 2 titles, the first of which is about communication access. Title 1 enforces producers of emerging technologies that have internet capabilities to enable their devices and services to be accessible to people with disabilities. In the next blog post, we’ll look at Title 1, who it applies to and what rules businesses will need to adhere to in order to get up to standard. We’ll be posting each post in this series every Monday – so stay tuned! d Introduction to the CVAA and its Regulations",neoadmin,4
2015-10-12,http://blog.neospeech.com/2015/10/12/title-2-of-cvaa-video-programming/,The Basics of Title 2 of the CVAA: Video Programming,", we took a look at Title 1 of the 21st Century Communications and Video Accessibility Act (CVAA), which states that advanced communication services and products must be able to be used by people with disabilities. In this blog post, we’re exploring Title 2, which determines how video programming must be easily accessed and consumed by people with disabilities. Title 2 of the CVAA requires that “information provided in video programming be made accessible to individuals who are blind or visually impaired and that certain apparatus be capable of delivering video description and emergency information to those individuals.” In layman’s terms,  The responsibility of updating to these regulations is on the manufacturer/provider of the product or service. This means that if you’re a business who is involved in video programming, it is your responsibility to get up to date and adhere to the regulations of the CVAA. Title 2 is primarily concerned with adding closed captioning to video programming, enabling disabled users to have access to those captions, and allowing users with disabilities to get emergency information that is presented during video programming on a range of devices. In the words of the FCC, Title 2 of the CVAA: There are quite a few aspects to this Title of the CVAA and some regulations are a little confusing. So, let’s take a look at a few of these in more detail. The first part to note is the first bullet point above, which “requires   (does not cover programs shown only on the Internet).” This means that all video programming that is initially shown on TV and had closed captioning must also have closed captioning when it is distributed on the internet. Note that this does not apply for video programming that is solely shown on the internet and was never broadcast on television. Here is a breakdown of the specific rules for the captioning of video programming: Note that these regulations only apply to video programming equipment, products and services that reach end user consumers.  These regulations came into effect on September 30  2012 for prerecorded TV programming and on March 30  2013 for live and near live programming. The next important part of Title 2’s regulations is the requirement for access to video programming guides and menus on navigational devices. The CVAA requires  Basically, this means that video programming devices that allow navigation, such as remote controls, need to be able to be used by blind or visually impaired users. Also, any products that include a navigational menu, such as set-top boxes, need to be accessible to blind and visually impaired users. These navigation devices need to have a specific button, key or icon which is used solely for activating closed captioning and video description. These regulations also apply to video programming distributors and the hardware and software manufacturers of multi-channel video programming (MPVD) distributors. However, it is important to note that only features that are required for video programming and user guides are covered under these regulations. This part of the CVAA was formally put in writing on May 30, 2013. However, the FCC has allowed affected entities with 3 years “to begin placing in service devices that comply with accessibility requirements related to on-screen text menus and guides.” This makes the current due date the 30  May 2016. However, this is subject to change by the FCC, so if this part of the act applies to your business, be sure to look out for updates on this matter. One of the main priorities of the CVAA is to make emergency information and communication accessible to people with disabilities. You may have noticed that Title 1 mandates Text-to-911 for communication devices to enable users with disabilities to effectively communicate with emergency services. Title 2 also includes new regulations that are designed to increase access to emergency information for disabled users. The CVAA requires “that emergency information provided in video programming be made accessible to individuals who are blind or visually impaired and that certain apparatus be capable of delivering video description and emergency information to those individuals.” Effective November 30, 2015 the FCC is  These regulations: In layman’s terms, this regulation requires that emergency information, such as severe weather warnings, is accessible for all users with disabilities. This means that emergency information must be conveyed both aurally and visually so that users with hearing or visual impairments can access the information. For example, If a program shows emergency information visually (such as in the use of an on-screen crawl across the bottom of the screen) they must also deliver this information aurally. This must be done through the use of a second stream, which will host the audio version. The emergency information must be played at least twice in full and must supersede all other information (such as translation into another language, for example). Also, you can use text-to-speech software to do achieve this. These rules also apply to the producers of advanced communication services as products.  The CVAA states that companies must ensure “that apparatus on which consumers receive, play back, or record video programming are capable of accessing emergency information and video description services.” Basically, the FCC has mandated that all devices in which emergency information could be consumed must have the capabilities to communicate emergency information with users with disabilities. This includes tablets, smart phone, laptops and any other technology that enables the viewing of video programming. For video producers who need to implement a secondary audio stream for emergency information, this needs to be in place by the 30  November 2015. For producers of advanced communication systems who have to make the secondary audio stream available on their devices, this will not be effective until June 2017. That’s a good question! Remember it is your responsibility as the manufacturer or distributor to ensure that your products and/or services comply with the regulations of the CVAA whenever necessary. According to the FCC, “the new emergency information requirements apply to video programming provided by entities that are already covered by Section 79.2 of the Commission’s rules–i.e.,  ” So, if you produce or deliver video programming content for personal consumption, then yes, the regulations of the CVAA apply to you. This includes (but is not limited to) emerging technologies including: If you are unsure if your product or service has to comply with the CVAA’s regulations, you can   to discuss your business needs. Absolutely! As Text-to-Speech providers, we know this seems a bit biased, but honestly you should. Text-to-Speech (TTS) is usually the easiest and cheapest option to implement, and probably the fastest too. To add to this, text-to-speech engines are dynamic and highly flexible meaning that they can be used to automatically find new information and read this aloud. For example, we can help you set up your TTS software so that it automatically reads out the information that is presented visually on during your video programming. Text-to-speech technology can also be embedded in smart phones, remote controls, set-top-boxes, tablets, laptops and many other emerging technologies to provide access for disabled users. The alternative is hiring a voice actor to read out your video program, broadcast or menu, which not only sounds incredibly boring for them but also very expensive for you. Not only is a text-to-speech solution cheaper, but it also runs automatically so you don’t have to worry about whether or not you’re meeting the required standards. Also, our voices don’t need sleep – they can work whenever your program is. Learn more about   or check out our   to learn about TTS online. Please feel free to fill out our   if you have any questions, inquiries, comments or would like to try a free demo. d If you’d like to learn more about the CVAA, Title 2 and whether or not it applies to you, here is a list of valuable resources: d Next time we’ll explore what your business need to do to ensure you are up to date with the FCC’s accessibility regulations for both titles of the CVAA. Next Monday’s post will be our last post in this series – so stay tuned! d The Basics of Title 2: Video Programming",neoadmin,4
2015-10-05,http://blog.neospeech.com/2015/10/05/title-one-of-cvaa-communication-access-for-disabled-users/,The Basics of Title 1 of the CVAA: Communication Access,"and the regulations within it. In this blog post, we’ll take a closer look at Title 1 of the CVAA. First, we’ll explore what Title 1 is and who it applies to before diving into what your business will need to do if your product or service needs to adhere to Title 1’s regulations. Title 1 of the 21st Century Communications and Video Accessibility Act (CVAA) “requires advanced communications services and products to be accessible by people with disabilities”. So, what is an “advanced communication service”? According to the FFC, advanced communication services and products are defined as having the following abilities: Basically,  “Advanced communication services” include all technological devices that utilize Wi-Fi or 3G, or connect to the internet in some way. If your device allows people to check their emails, receive and send text messages, watch videos or interact with the world through an internet connection, your device must adhere to the following regulations. This covers a huge  , including: If you are unsure if your product or service is classed as an “advanced communication service or product”, we would recommend that you contact the FCC. If you’d like to contact the FCC, you can find their contact information  . 
 As you now know, Title 1 enforces businesses that create products or services for end-users, such as laptops, tablets and smart phones must make their equipment accessible to those with disabilities. It is important to note that Title 1 of the CVAA will affect every business differently. For example, one of the most important parts to note for smart phone manufacturers is the last bullet point – the Text to 911 capabilities. This aims to enable everyone, including those with disabilities, to contact and communicate with emergency services. However, if you’re a computer manufacturer, you may be mostly interested in the first bullet point where you have to provide a mechanism for disabled users to access the internet on your device. Note that multiple points may apply to your business. The FCC has decided it is your responsibility to discover which parts of the CVAA you must comply with. The last blog post of this series will go through what businesses need to do to get up to date with the CVAA’s regulations. However, if at any stage you’d like clarification on what parts of the CVAA apply to your business or whether your product is classed as an “advanced communication service or product”, you can  . d Title 1 of the CVAA enforces producers of “advanced communication services and products” to enable their devices and services to be accessible for people with disabilities if the product or service can connect to the internet. This will affect every business in a different way depending on what product or service you produce. If you’d like to learn more about the CVAA, Title 1 and what parts of the title you need to adhere to, here is a list of valuable resources: g At NeoSpeech, we specialize in offering the best quality text-to-speech solutions. Text-to-speech (TTS) software does exactly what it suggests – it takes text and turns it into speech. To learn more about TTS, check out our   post. TTS software is a great technology to use to enable your product or service to be accessible to people who are blind or have other visual impairments. Text-to-Speech can be used to comply with many parts of Title 1. For example, Title 1 of the CVAA requires internet browsers built into mobile phones to be accessible and usable by individuals who are blind or visually impaired. If you produce smart phones, tablets or another emerging technology that enables access to internet browsers, you could incorporate text-to-speech technology to read out what is presented on the screen and instructions for how to navigate the device. If you’re interested in learning more about NeoSpeech’s Text-to-Speech and whether we can help your business get up to standard, please feel free to   or contact us through our   form and our friendly team will be happy to help. g Next time we’ll be examining the other title in the CVAA, Title 2, which describes the accessibility regulations around video programming and uploading videos to the internet. We’ll be posting each post in this series every Monday – so stay tuned!",neoadmin,4
2015-10-09,http://blog.neospeech.com/2015/10/09/sign-language-glove-enhances-communication/,"One Small Sign Language Glove, One Giant Leap for People with Disabilities","A breakthrough for people with disabilities is on its way –  . The glove, which has been named SignLaguageGlove, recognizes hand gestures and turns them into text on a scrolling screen. It can also use text-to-speech software to turn the gestures and text into audible content. This is a fantastic development for people with spoken language or hearing disabilities, enabling much easier and faster communication with people without disabilities and with people with different disabilities to their own. The glove was developed by artist and designer, Hadeel Ayob. Hadeel Ayoub recently completed her MA in Computational Arts at Goldsmiths in the university’s Department of Computing. She developed the glove in an endeavor to improve communication between people with different disabilities. Hadeel Ayoub says,  In an interview with  , Ayoub revealed that her motivation behind developing the glove was originally her autistic niece.  This fantastic technology will not only help people with autism, but people with hearing and visual disabilities as well. The glove has 5 flex sensors that correspond to the 5 fingers of a hand. These detect bends and curvatures of the hand as the user signs and reports the values to a serial monitor. Concurrently, an accelerometer is used to detect the orientation of the hand and the direction of pointing fingers. Using this information, a computer program in the glove identifies the output values of the sensors and accelerometers and works to match them to a series of statements. The most appropriate statement is displayed on the screen and, in the latest version of the glove, read out loud using a text-to-speech chip. d Ayob has created 3 different prototypes with a 4  edition on its way. In each of these developments the hardware was incorporated more smoothly into the glove and the software was improved. As you can imagine, the first of her prototypes was rather basic and took up a lot of room, at least when compared to her further renditions. It looked basically like a traditional glove for cold weather with a range of wires leading to and from the glove. The glove could only display 4 characters on the screen at a time and it needed to be attached to a computer – not ideal for going to the shop to buy your groceries. 1  prototype of the SignLanguageGlove The 2  edition had a more useful design with a smaller microcontroller with a scrolling screen, meaning that words larger than 4 letters could easily be displayed and read. Best of all, the 2  edition was wireless, increasing everyday accessibility so you easily take it to the supermarket to aid in asking where the milk is.  Hadeel explains. “ 2  Prototype of the SignLanguageGlove The 3  edition of the glove has a much sleeker design. The wires are sewn into the glove and once again, the software and hardware got an upgrade. One feature we are excited about is the inclusion of text-to-speech software into the glove, enabling the SignLanguageGlove to read out loud the text that was being produced by the movement of the glove. This adds a whole new level of accessibility to the glove, enabling the glove to be easily used by people with disabilities to communicate directly with people who do not understand sign language without the recipient having to read a small screen. 3  Prototype of the SignLanguageGlove The addition of text-to-speech technology also brings down a unique barrier that hasn’t been able to be overcome this easily ever before. This feature allows the blind to hear the deaf – enabling communication not only between people with and without disabilities, but also between people with different disabilities. The 4  edition of the glove, which is currently being developed, will include Wi-Fi and smart phone apps to enhance the usability of the glove even further. The apps will allow the output of the glove to be translated into languages other than English, which is currently the only language the glove can be used for. Ayoub speaks Arabic, French and English and wants to enable people with disabilities to do the same if they want to. Ayoub also wants to develop a version of the sign language glove for kids, which will be no easy feat. All the components will need to be scaled down and become lighter so that they will not only fit on a child’s hand but also be an appropriate weight for a child to lift. The 4  prototype is expected to  , but Ayoub hopes that people with disabilities will not have to pay for the gloves themselves. Instead, she hopes that government organizations, schools and workplaces will recognize the importance and need for the glove and buy them for workers or students with disabilities.  d If you’d like to learn more about Ayoub’s SignLanguage Glove,   to see Goldsmith’s article on her work or visit   about the project. Or if you’d like to learn more about other cool applications of text-to-speech technology or discover how to integrate TTS into your next project,  .  d",neoadmin,3
2015-10-19,http://blog.neospeech.com/2015/10/19/what-your-business-needs-do-to-get-up-to-date-with-the-cvaa/,What Does Your Business Need to Do? How to Get Up to Date with the CVAA,"In the last few blog posts, we’ve looked at both titles of the CVAA and the regulations that businesses must adhere to if they are included. In this post, we’ll discuss how to figure out if your business needs to adhere to the rules of the CVAA and what you need to do to get up to date. Firstly, let’s recap what the CVAA is. The 21  Century Communications and Video Accessibility Act (CVAA) was passed in 2010 to enable people with disabilities to have access to the emerging technologies of the 21  century. It is broken down into 2 titles, each of which is briefly described below. Title 1 determines that advanced communication products and services that utilize broadband must be accessible and usable by people with disabilities, including smart phones, tablets, laptops, gaming consoles and more. This part of the CVAA also describes the requirement for these products to have Text-to-911 capabilities. Learn more about title 1 in our   post. Title 2 aims to enable users with disabilities to consume video programming on television and on the internet easily. Title 2 also increases accessibility to emergency information by requiring a second audio stream to be dedicated to the aural presentation of emergency information if it is not aurally presented during the main video program. Learn more about title 2 in our   post. In short, the CVAA was designed to ensure that all emerging technologies of the 21  century are easily accessible by people with disabilities. So let’s answer the question on everyone’s minds – does this apply to my business and if so, what do I have to do to get up to date? Step 1 is figuring out if your business needs to adhere to any of the parts of the CVAA. This is important to do because it is   to discover if these regulations apply to your business. To help, we’ve put together a list of products and services that are included in the CVAA’s regulations. Title 1 of the CVAA   According to the FFC, advanced communication services and products are defined as having the following abilities: Basically, advanced communication services and products include all technological devices that use Wi-Fi or 4G, or connect to the internet in some way. If your product allows people to connect to the internet, check their emails, or send text messages then your device must adhere to the CVAA’s regulations. Third party apps that you download to your smart phone or tablet are currently NOT included in Title 1’s regulations. Only what comes on the phone or advanced communication product when it arrives at the end-user needs to be accessible. However, we’d recommend keeping an eye on this because there are challenges to this occurring that aim to make third party apps inclusive in these regulations. Title 2 enforces video programming producers to enable their content to be consumed by people with disabilities, along with providing reasonable access to emergency information. So, if you produce or deliver video programming content for personal consumption, then yes, the regulations of the CVAA apply to you. This includes (but is not limited to): Note that this is not a comprehensive list. There are far too many emerging communication-based products and services that are now available to list them all. If you are unsure if your product or service has to comply with the CVAA’s regulations, you can   to discuss your business requirements. So, you’ve discovered that your product or service does fall into the list of advanced communication services or is used to convey video programming on the internet. Now what do you need to do? The responsibility for providing accessibility for disabled users is in the hands of the business manufacturing the product and providing the service.  . If this is not achievable, it is acceptable to enable your product to be compatible with other accessibility solutions, such as hearing aid compatibility. However, you must be able to prove it was not achievable to add the accessibility features to your product or service. You need to ensure that visually and hearing impaired users of your device can have the best possible access to all functions on the device.  If you’d like to learn more about what disabilities are covered by the CVAA, take a look at our 1  post:  . Also, you can visit our previous blog posts to learn about the specific regulations for Title 1 and Title 2. As part of the CVAA, the manufacturers of advanced communications services and products and of video programming related services and products must maintain records as evidence that they have complied with the CVAA. Key things to note about the record keeping rules: Entities that must comply with these new regulations  that states that records are being kept in accordance with the CVAA. To learn more or to find out how to submit your annual record keeping certification,  . See the full report on the Record Keeping Requirements from the FCC  . Well,  . This is when the record-keeping requirements came into effect, so make sure that if any of these regulations apply to your business that you are keeping well-documented and clear evidence of your compliance to the CVAA. Most of Title 2’s regulations, such as the  , so hopefully you have got up to date on these. If you’re a producer of video programming for TV and internet consumption and you haven’t looked into these regulations, we’d recommend taking a look at our blog post specifically about Title 2 of the CVAA here. (link to second blog post). However, some parts of the CVAA were granted extensions. For example, producers of e-books (such as Amazon’s Kindle) petitioned to be granted an extension. This was granted – so   to get your product into compliance. Also, if you’re a  . Originally, the FCC wanted the deadline to be in May 2015, but the deadline got pushed back. If you’d like to learn more about why the deadline was pushed back,  . 
 Some extensions or waivers are very specific and as such, we will not be listing all of them here. For example, baby monitors which enable video recording do not have to have closed captioning options and have been made except from the CVAA’s regulations. If you’re unsure if your product or service needs to adhere to the CVAA’s regulations, you can contact the FCC to discuss it. If your product or service is found not to be complying with these new regulations, you can be fined up to $100,000 per day for violations, up to a maximum of $1 million. The FCC can also order you to bring your products into compliance. This can all seem rather daunting, especially if you’re selling a product or service that was not designed for disabled users at all. If these regulations do apply to your business, you might have to go back to square one and discover how to integrate accessibility features into your product and service. However, here is a curve ball you should note – accessibility is only required if it is “achievable” (in the words of the FCC). But what exactly is “achievable”? To determine what is “achievable” the FCC looks at 4 factors: This means that if you cannot add accessibility features to your product or service, you might be granted discretion and allowed to continue selling your product or service without meeting the CVAA’s regulations. However, if you claim your product or service is in the “un-achievable” category, it is up to you to prove this to the FCC. Definitely! Text-to-Speech (TTS) technology offers a range of benefits, including: At NeoSpeech, we offer a range of solutions, including our TTS Cloud Service for access to an online text-to-speech engine and a variety of downloadable text-to-speech solutions that can be customized to your product or service. The alternative is hiring a voice actor to read out your video program, broadcast or menu, which not only sounds incredibly boring for them but also very expensive for you. Not only is a text-to-speech solution cheaper, but it also runs automatically so you don’t have to worry about whether or not you’re meeting the required standards. Also, our voices don’t need sleep – they can work whenever your program is. You can learn more about Text-to-Speech and our TTS solutions here: d What Does Your Business Need to Do? How to get up to date with the CVAA",neoadmin,4
2015-11-06,http://blog.neospeech.com/2015/11/06/top-5-everyday-uses-text-to-speech-technology/,Top 5 Everyday Uses of  Text-to-Speech Technology,"Sometimes, people ask us if there are any uses of Text-to-Speech (TTS) technology for personal users who don’t have a disability. While Text-to-Speech can be hugely beneficial for people with physical or learning disabilities, there are many other uses for TTS out there that people without disabilities can benefit from too. Whether you’re commuting and want to listen to an audio book or getting directions from your GPS, there are tons of ways you can use TTS in everyone’s everyday life. 
 I’ll use yesterday as an example. Yesterday, I interacted with Text-to-Speech in at least 5 different scenarios. And I’m not alone – over 50% of the world’s population relies on TTS every day! On my way to work, I used my GPS to find a route without traffic. At work, I proof-read one of my blog posts to make sure I wasn’t making any silly spelling mistakes. At lunch, I went to an ATM to check my account balance. On the way home, I plugged my phone into my car stereo and listened to the latest news. For dinner, I asked my phone to find a pizza place close to home. Sounds like a normal day, right? So, where does Text-to-Speech come in? In every one of these situations I used Text-to-Speech software. My GPS used Text-to-Speech to tell me where to turn and my ATM used Text-to-Speech to tell me what options I had for checking my account balance. At work, I used our TTS software to proofread my article, which is a great idea, because it reads your article out loud so you can hear if a sentence is too long or a word is misspelled. I’m sure you all know how hard it can be too see mistakes when you’ve read your work 15 times already.     In hindsight, it was amazing how many times I’d used Text-to-Speech software without really realizing it. So, I wanted to bring to you the   in this blog post so you can start making the most of this great technology. You know when you’re running late for a flight and they embarrassingly call your name out to the entire airport? Or maybe you’re an organized person who just listens in to know when your flight is boarding? Either way, these are prime examples of everyday uses of Text-to-Speech. One of the great aspects of TTS is that it is dynamic, so it can actively change to announce a different gate number or passenger’s name, which just wouldn’t be possible with a pre-recorded audio file. This means the staff can focus on more important things, such as getting you from A to B. This isn’t limited to the airport, either. TTS is used in buses, in GPS sys tems , ti c keting apps and so much more. It is even used in interactive audio kiosks at train stations. Some cities have free apps that will give you live updates about the running of trains or buses – these usually use TTS. You may have never noticed before, but TTS is all around you as you commute. After a long day at work, it is time to give your eyes a break. Whether you’re on the train, biking or in your car, TTS can be used to listen to the latest news without having to squint to see a small screen.        This doesn’t just apply to consuming the most recent news either; you can also relax and take in the latest horror story or romance novel. Audio books are an excellent example of technology that uses Text-to-Speech. You can sit back, relax, and listen to the entire story with your eyes closed. Text-to-Speech can handle PDFs and Word Documents too, allowing you to study without straining your eyes. As I said earlier, TTS is perfect for proof-reading your work by enabling you to hear your writing out loud and identify mistakes. Our TTS products highlight the sentence and word that is being said aloud so you can easily spot a mistake and fix it. But Text-to-Speech can help you learn in many other ways too. Our TTS has a variety of speeds so you can slow the pronunciation down to a level at which you can hear exactly how a word is said. This is useful if you find a new word you don’t know how to pronounce or are practicing a speech and want to hear how a phrase would be said naturally.       Have you ever received an automated voice message from your doctor’s office reminding you of the timing of your next appointment? This usually uses Text-to-Speech because the TTS engine can dynamically call each patient and insert the appropriate name and time into the message. So listen out next time you get a phone call from your doctor and see if you can spot the use of TTS. On the other hand, there are numerous exercise apps that utilize Text-to-Speech technology. These guide you through your exercise routine so you don’t have to look down at your device while running. These apps can even give you instructions while listening to your favorite song, which is perfect for keeping up your pace. Lastly, virtual personal assistants are becoming more and more popular over the last few years. These are virtual people who aid you in certain ways, from reminding you to take your pills to helping diagnose your illness without having to see a doctor.     I think one of the most rewarding everyday applications of TTS is the ability to help us learn. When learning a new language, it is essential to hear a native speaker pronounce each word clearly so you can follow along and practice. However, paying for a tutor can be expensive and inflexible. The internet has opened up new possibilities for learning a language online. There are countless websites that offer an introduction to your new favorite language. But how do they get clear and friendly voices to say all those vowels and words for every language with high quality audio? The answer is TTS. By using an application with TTS, you no longer need to meet a tutor on their schedule. You can learn when and where you want for a fraction of the cost. Want to try it out for yourself? Take a sentence from your Spanish textbook, type into our VoiceText™ Text-to-Speech desktop engine and viola! Violeta, our female Spanish voice, will let you know exactly how to say that word or sentence. Or, if you don’t have our VoiceText Engine, try out the free demo at the top of our  .   .   .",neoadmin,9
2015-11-20,http://blog.neospeech.com/2015/11/20/warning-neospeech-voices-found-on-scam-site/,Warning: Do Not Download NeoSpeech Voices from This Scam Site,"We recently discovered that our text-to-speech voices are allegedly being offered on  . These are NOT legitimate versions of our voices and will cause harm to your computer. When we downloaded these voices, our anti-virus software detected 4 viruses just after the installation process was complete. Also, you may have noticed that this website claims to have a homepage, a blog and other useful elements but if you click on these buttons, they will lead you nowhere. Hence, it is our understanding that this is a scam and could cause serious harm to your computer if you try to download NeoSpeech voices from this website. Please take care. If you’d like to talk to us about any of our text-to-speech voices please feel free to fill out the   and one of our friendly sales team members will answer any questions you have.",neoadmin,6
2015-11-18,http://blog.neospeech.com/2015/11/18/day-life-forensic-speech-scientist/,A Day in the Life of a Forensic Speech ScientistWhat is Forensic Speech Science?Introducing Dominic Watt and How He Became a Forensic Speech ScientistThe Accent and Identity on the Scottish/English Border (AISEB) Project,"It isn’t everyday that you meet a Forensic Speech Scientist. Yes, a  . Dominic Watt is a senior lecturer of Forensic Speech Science at the University of York in England. Dominic, who is one of the world’s leading professionals in his field, has been working on a project called the  project, which examined the difference in the English spoken on either side of the Scottish/English border and the link between personal identity and accent. So, how does accent influence our first impressions of people? How does accent relate to national identity? Does this change along the border? Does the accent border follow the political border? These are just some of the things that Dominic and his incredible team explored. In today’s blog post we’ll explore what forensic speech science is and the life of a forensic speech scientist. We’ll dive into the AISEB project to discuss the findings and implications of this research and the interesting link between national identity and accent. i According to the  ,  . Forensic speech science enables certain linguists and speech scientists to provide evidence in legal proceedings. For example, a forensic speech scientist may be used as an expert witness in a case to determine if two brands names sound too similar. It is a beautiful combination of law, speech technology, linguistics and phonetics; think linguistics meets CSI. i After talking with Dominic for only a few minutes I could tell he loves what he does. Dominic had always been interested in language from a very young age. Like his parents, Dominic decided to study linguistics at university. He taught in the UK and Germany before coming back home to start a PhD in Phonetics. Then, Dominic was asked to be a part of a project that would change his career. His first case ended positively and began his career in forensic speech science. Dominic was intrigued and agreed to take on the case. We discussed this case for a little while and in the end, the two brands were deemed too similar and the second brand was ordered to make a change. After about 20 more minutes, he finished nicely with: One of the main things I wanted to talk to Dominic about was the  . This project was primarily run by a group of forensic speech scientists from the University of York, which included Dominic Watt. The AISEB project looked at one of the greatest concentration of linguistic features in the entire English speaking world – the Scottish/English border, which you can see below as a red dotted line. g Along the border, the national identity of inhabitants of towns on either side is rather complex. Many people consider themselves undoubtedly English while others are certain they are Scottish, and there are many who lie somewhere in the middle. Language plays a central role in how these people mark their identities, but before the AISEB project it was unclear which features of pronunciation marked ‘Scottishness’ and ‘Englishness’ and whether people with these features identified themselves with the nationality that their accent and pronunciation suggested. i The project also examined how closely the political border followed the linguistic border, how an accent affects a person’s perception of someone they don’t know, and the difference between the east and western side of the border in terms of strength of an accent. What follows are the main highlights from the interview I had with Dominic about the AISEB project. i He went on to describe the unusual history of Berwick-Upon-Tweed (often shortened to Berwick), with the following points being my favorite historical peculiarities: The AISEB team decided to look into Berwick, along with several other towns along the border and examine how such stark linguistic differences could exist and how they influence the identity of the people from those towns. p Dominic explained their methodology of visiting each of the towns along the border and asking the inhabitants a series of questions. These questions were designed to encourage participants to demonstrate their accent and pronunciation of different vowels and consonants to discover these fine-grain speech patterns.   He had a fantastic accent from my perspective, and although you can’t hear it, the R he is referring to is that typical Scottish R, which is very pronounced and distinctive. While I was impressed at the project’s ability to demonstrate these “ ” as Dominic called them, it was even more fascinating to hear about the project’s more unpredictable findings. For example, the AISEB project not only looked at differences between the north and south side of the border, but also between towns along the border at the western and eastern ends. On the North side of the border (in Scotland), they looked at Eyemouth and Gretna and on the South side of the border (In England) they examined Carlisle and Berwick-Upon-Tweed. Here you can see the Scottish/English border as a red dotted line with each of the areas examined highlighted in red. u He described how people from Gretna would often get told that they “don’t sound Scottish” or that they couldn’t possibly be from Scotland because of their rather English-sounding accent, despite Gretna being located in Scotland. On the other hand, people from Eyemouth sounded very clearly Scottish. Then we discussed the western side of the border more thoroughly, with Carlisle on the English side and Gretna on the Scottish side. He discussed two small towns on the border and the intense difference between the accents in each town. The towns are so close that “ ”, Dominic said. Everyone said to Dominic’s team that if you were on one side of the river, the accent was very Umbrian but on the other side it was clearly Scottish. He pointed out that this is particularly interesting when you realize that people are free to move between England and Scotland because they are both part of the United Kingdom. This means there is no legal reason why the Scottish people have to stay on one side and the English on the other, but they choose to do so. The AISEB team was intrigued by this and wanted to find out why this happened. Dominic and his team chose to run a series of experiments where they invited people to respond negatively or positively to short speech samples. These speech samples were deliberately chosen to evoke “Scottishness” or “Englishness”. Then the team asked the participants to discuss their view of the person who was speaking in the recorded audio. At one point, they asked the participants to identify whether one particular speech sample sounded Scottish or English and why.  j w They discovered that accent acted as a gatekeeper to a perception of a nationality. This was shown in first impressions, where a person would automatically assign traits to a new person depending on the accent they heard. If the accent they heard was Scottish, for example, they would expect that person to be a Scotland supporter, despite not knowing much about them at all. And when this perception of a person based on their accent did not match with their activities, people would get confused. e One of Dominic’s students, Georgina Brown, is developing an automatic accent classifier system. One of the uses of this is to be able to automatically discover if a person is from a certain country or region based on their accent. Georgina’s automatic accent classifier is an innovative use of speech technology, which could expand the realm of applications of forensic speech science. I am planning on interviewing Georgina in the near future and writing a post to share her interesting innovations with you, so stay tuned. o u Be honest, you were sold at linguistics meets CSI. I’d love to hear what you think – please email me at   if you have any questions or start a discussion in the comment section below. p e The data from the AISEB project can be used to aid in other researchers’ investigations upon application. If you’re interested in working with the AISEB project data, please contact  . If you’d like to learn more about forensic speech science, Dominic Watt, or the AISEB project, feel free to take a look at any of these resources: e If you’d like to learn more about NeoSpeech and our speech technology, feel free to check out the   here or fill out our   form and one of our friendly team members will be happy to help.",neoadmin,0
2015-06-08,http://blog.neospeech.com/2015/06/08/announcing-our-new-text-to-speech-blog/,Announcing our New Text to Speech Blog!,"This blog will explore advancements in the speech technology industry with a specific focus on   (TTS) technology and its applications. We aim to become a comprehensive resource for all speech technology ideas and improvements. Our first post will highlight how   can change lives. A young mute bride named Ashley shares her story with us and her mother explains how NeoSpeech’s TTS technology improved Ashley’s quality of life by allowing her to communicate freely with those around her. We look forward to bringing you this heartwarming story, along with many other exciting posts about TTS technology. If you have any ideas or stories you would like to share with us that could be featured on our blog, please feel free to contact us at",neoadmin,4
2015-11-25,http://blog.neospeech.com/2015/11/25/fccs-deadline-for-text-to-speech-is-approaching/,FCC’s Deadline for Text-to-Speech is Approaching: November 30th 2015,"Is your business ready for the FCC’s new Text-to-Speech rule? Are all your products easily accessible for people with disabilities? The deadline for compliance to the FCC’s Text-to-Speech rule is November 30 ! The Text-to-Speech rule, which is also known as the Audible Crawl rule, requires television broadcast stations and multi-channel video programming distributors (MVPDs) to enable all emergency information to be easily accessible to people with disabilities. This is part of Title 2 of the 21  Century Communications and Video Accessibility Act (CVAA). The CVAA was passed into law in 2010 to enable people with disabilities to use emerging technologies, such as smart phones, tablets, smart TVs and more. Many regulations within the CVAA have come into effect already, but the Text-to-Speech rule doesn’t come into effect until the end of this month. Are you prepared? d Effective November 30  2015, the FCC requires broadcasters and MVPDs to use “ To be clear, these regulations: d For example, if an episode of a popular television show is currently showing and a broadcaster displays an emergency weather update scrolling across the bottom of the screen, the emergency information must be accessible on a secondary audio stream. The broadcasters would also need to present an aural tone, usually a three second tone sound, on the main audio stream that alerts individuals that they can listen to the secondary audio steam and play the emergency information at least twice in full. d The FCC defines emergency information as   This includes: d However, the FCC is rather vague about what constitutes as emergency information, so we recommend implementing a secondary audio stream for all information that could be considered emergency information or contacting the FCC if you are unsure. d Definitely! Text-to-Speech takes all the hassle out of creating and maintaining a secondary audio stream. The whole system can be set up to run automatically, day and night, to ensure that your business and broadcasting is always up to standard. NeoSpeech’s Text-to-Speech is perfect for automatically and seamlessly providing audio emergency updates with high quality, human-like synthesized voices. We take all the hard work out of providing a secondary audio stream for your broadcasts. Contact our sales team through the Sales Inquiry page and they’ll be happy to help you implement a Text-to-Speech solution or answer any questions you have. d If you are unsure if your product or service has to comply with the CVAA’s regulations, you can   to discuss your business needs or contact us to discuss any questions you might have. To learn more, you can take a look at our in-depth articles on the CVAA below or you can   to see the Official Guide to the CVAA. 
 d d",neoadmin,4
2015-07-22,http://blog.neospeech.com/2015/07/22/buddy-the-in-home-robot-uses-text-to-speech/,Will Buddy be Your New Best Buddy? In-Home Companion Robot is on Its WayCustomize your Buddy with AppsArtificial Intelligence is the FutureNeoSpeech and Artificial Intelligence,"Blue Frog Robotics, a French technology start-up, is creating social robots that are designed to live in your home. Buddy, an in-home companion robot, is programmed to protect your home, assist with daily tasks and talk with members of your family. Who doesn’t want a little robot to do the vacuuming? d d But how could Buddy do the vacuuming? Buddy’s body is covered in sensors and cameras that allow it to detect objects on the floor and move around them. Buddy has a Butler Mode, in which it can fetch and clean objects. In Security Mode, Buddy will patrol the outside of your home for danger. Buddy can even be used as a calendar, keeping track of important dates and reminding you about them. In order to communicate with your family, Buddy uses Text-to-Speech (TTS) software. Buddy can wake you up in the morning, say hello, and can be instructed not to leave until you get up. Buddy can even be asked to tell a bedtime story to children. As a Text-to-Speech company, it is exciting to see TTS being incorporated into humanoid robots. We can’t wait to hear Buddy talk!   ,  Blue Frog Robotics is aiming to create a range of interesting apps that you can download to Buddy. Buddy can connect to your wireless internet to fully integrate into your home and enable the downloading of apps. Frack de Visme, the CFO of Blue Frog Robotics understands the importance of offering a range of entertaining and educational apps to consumers.   he said. Ever since the first sci-fi movie featuring a humanoid robot, humans have dreamed of creating companion robots. Buddy is the first step towards realizing that dream. Artificial Intelligence, or AI, is the future of Speech Technology as we develop robots that we can talk to and enable them to talk to us. Frank de Visme believes that “in five years many people will have personal robots”. We are looking forward to seeing if this comes true.   . Frank was designed for a very different purpose to Buddy. While Buddy is designed to help you in your daily life, Frank was designed to enable scientists to make cutting-edge scientific and medical advances. Frank can not only have a conversation with you using TTS, but he also has a working heart, arms, hands and much more. Frank even has a working pancreas. Many of Frank’s organs are prototypes for creating artificial organs for humans, which could change the course of a human life dramatically.   “What if an artificial heart can buy us more lifespan – what will a society like that look like?” asks Bertolt Meyer, the host of a documentary on Frank. “The heart is already being used in patients”. NeoSpeech’s VoiceText™ Text-to-Speech has also been successfully incorporated into other humanoid robots, primarily in Japan for the Japanese language. We look forward to seeing this market grow and to see the positive influence these advancements can make on human life. Want to learn more about our Text-to-Speech? Take a look at our   to see our TTS packages for software developers and personal users. Text-to-Speech has a huge range of applications and robotics is only one of them. Check out our   to discover the different ways in which our TTS can help you or your customers.   .",neoadmin,4
2015-07-08,http://blog.neospeech.com/2015/07/08/neospeech-voices-rank-1-and-2-in-voicedream-texttospeech-app/,NeoSpeech’s Voices Rank #1 and #2 in VoiceDream Text-to-Speech App,"We believe our voices are the best quality synthesized voices on the market  VoiceDream, a leading Text-to-Speech app for iPhones and iPads, offers a range of NeoSpeech’s voices as in-app purchases. The app offers voices from many of the world’s best Text-to-Speech providers, including NeoSpeech, Ivona and Acapela Group. Available in over 86 countries, it is said to be the most widely accessible Text-to-Speech app in the world.  As you can see in the picture below, our James and Paul voices are ranked #1 and #2 on the list. , including 4 of our US English voices (James, Paul, Julie and Kate) and our latest UK English voice (Bridget). NeoSpeech has the same number of Top Selling voices as Ivona and Acapela Group combined, who have 2 and 3 voices in the Top 10 list respectively. We have always differentiated ourselves as being the most human-like, natural, high quality voices available and it is great to see that our end-users think the same way. d You don’t have to take our word for it! Check out what some of the VoiceDream customers have to say about NeoSpeech’s voices.  by AviddReader  With this feature-rich app, reading is a pleasure. If one is listening, there is a wide selection of voices, including the human-sounding Kate and Paul from NeoSpeech.  ★★★★★  by Kyle122436  I have ADHD and this app is a life saver with easy audio converting for reading assignments. NeoSpeech voices are incredible.  ★★★★  by Joe2k4  I use this app a lot for book share I really enjoyed reading with Paul now because he’s one of my favorite voices. . We’d love to hear from you! Feel free to reach out to us at   and tell us your NeoSpeech story and what you think of our voices. VoiceDream is just one of the many ways in which our Text-to-Speech can be used. Take a look at our   page to see the full range of applications of our TTS software.",neoadmin,6
2015-06-26,http://blog.neospeech.com/2015/06/26/text-to-speech-changes-lives/,Text-to-Speech Changes Lives: How Ashley Found Her Voice,"At NeoSpeech, one of the most uplifting applications of   is to aid people with communication related disabilities – like Ashley. This is Ashley. Like any other young woman she has a job, a home and independence. She is an intelligent young woman with hopes and dreams. She has a loving family who want to communicate with her and let her communicate with them. Unlike other young women, Ashley drowned at the age of 3. After being officially dead for 1 hour and 40 minutes, Ashley was in a coma for 2 months. When she awoke, she had lost her ability to talk. Luckily, Ashley was still able to hear. But many people thought she was deaf because she cannot talk and had to use American Sign Language (ASL) to communicate. Ashley was placed in Special Ed classes and had to have an ASL interpreter with her throughout her schooling days. That was until Ashley’s mother, Cathy, discovered NeoSpeech’s text to speech technology.  Cathy explained to us. Using NeoSpeech’s Text to Speech software, Ashley was suddenly able to communicate with the world around her without relying on ASL. By simply typing in what she wanted to say and pressing a button, she could engage in a conversation with anyone, including her grandparents. Ashley even used NeoSpeech’s text to speech software to get married! The software allowed her to say her vows out loud to her husband, just as any young woman would.    NeoSpeech’s technology is not only useful for people who are mute but also for people with other communication disabilities.   Cathy said, describing NeoSpeech’s TTS technology. We are honored to have played such an important part in Ashley’s life and to aid her with her wedding. It is wonderful to see NeoSpeech changing the lives of the people in our community. If you would like to learn more about our products, check out our product page at   or our online cloud service at  . Also, please feel free to contact us at any time.  Do you have a story you would like to share with us? Please feel free to contact us at   we would love to hear about how you use NeoSpeech’s text-to-speech technology!",neoadmin,7
2015-07-14,http://blog.neospeech.com/2015/07/14/what-is-text-to-speech-and-how-does-it-work/,"What is Text-to-Speech and How Does It Work?Okay, but how does Text to Speech work?","Some of our readers are linguistic or software geniuses with a complete understanding of Text-to-Speech software. But some of you may be relatively new to this part of speech technology and you may have found yourself wondering,  d  TTS stands for   (also written as Text to Speech) – a form of speech synthesis that converts text into voice output. Text-To-Speech software basically takes the text you write and turns it into speech files that you can use. From text to speech – nice and simple! d  There are numerous ways you can create audio from text. At NeoSpeech, we use a process called  The process starts on both ends— voice database building language text processing —that meets in the middle to produce speech. But for purposes of understanding, we’re going to break down into a simple 6 step process to show you how we create such high quality speech. First, we choose a voice actor with a great sounding voice who is fluent in a certain language. Then we bring him or her in to talk with us – for hours and hours and hours. We record the voice actor saying a range of speech units, from whole sentences to syllables. These can be recipes, sports results, magazine articles or anything that lets us capture the natural sound of the actor’s voice. This covers examples of all the possible sounds in a given language.   Now that we have thousands of recorded sound files, we need to sort them out and organize them. The speech units are labeled and segmented by phones, syllables, morphemes, words, phrases, and sentences. These speech units are used to build a large voice database. The voice is now ready for you to use.   You sit down at your computer and open one of NeoSpeech’s products. You type in the text you want transformed to speech. From the language processing end, your text is normalized and broken down into phonetic sounds before going through a series of analyses to understand the structure of the sentences as well as to determine the context of the word for pronunciation. This is called Natural Language Processing. Through these processes, we are able to establish prosody—rhythm, stress, and intonation—and produce natural sounding speech.   This is where the Natural Language Processing (NLP) Part and the Voice Database come together to start producing speech. Once the NLP is complete, our software searches the voice database and chooses the speech units that best fit together to produce the sounds associated with your text. This is called Unit Selection (hence the name, Unit Selection Synthesis).   Voila! After a bit of technical processing, you have your new audio file with one of NeoSpeech’s top quality voices.   d We use USS because it is considered to produce The other main speech synthesis technique used today is called HTS (HTS based speech synthesis system). So, which one is better? Basically, it is a trade off of  . Using USS takes a lot of time to produce the TTS engine. We need many hours of voice recording from the actor and then hours of computer programming. It can take up to a year from start to finish to produce a truly high quality text-to-speech engine. Despite this, we believe USS is worth the wait. It produces much higher quality audio than HTS based synthesis, which only requires between 30 minutes and 3 hours of voice recording and takes much less time to produce a new TTS engine.   Want to learn more about speech synthesis techniques? Keep an eye out for our next blog post:  This is a more detailed analysis of HTS and USS and the pros and cons of each one.   .    Lastly, feel free to check out our   page to see what TTS options we have to offer.",neoadmin,6
2015-07-24,http://blog.neospeech.com/2015/07/24/stephen-hawkings-speech-system-is-free-and-open-source/,Intel releases Stephen Hawking’s Speech System as Free and Open SourceHow Stephen Hawking’s Speech System WorksWhy make this technology Open Source?Who could this technology help?Stephen Hawking’s Text-to-Speech,"Stephen Hawking, the world-renowned theoretical physicist, found his voice in 1997. It was at this time that he partnered selectively with Intel to create his unique computer system that allows him to communicate. And now, thanks to Intel, many more people with physical disabilities will be able to communicate too. Professor Hawking was diagnosed with Amyotrophic Lateral Sclerosis (ALS) at the age of 21, which slowly paralyzed him and rendered him to a wheelchair. As he lost the ability to use his muscles, it was uncertain if he would ever be able to communicate again. In 1997, Intel designed a speech computer system that was integrated into Professor Hawking’s wheelchair. Utilizing Text-to-Speech functionality, Stephen Hawking could express his brilliant thoughts and ideas. The source code and programming code for Stephen Hawking’s speech technology have been closely guarded secrets by the proprietary laws forged by Intel in 1997 – until now. Yesterday, in an   Stephen Hawking’s Speech System is comprised of 3 main parts: 
Every time Professor Hawking moves his cheek, the infrared sensor on his glasses senses his movement and triggers a signal that goes to a software platform. The software platform allows him to do a range of tasks, such as using a virtual keyboard interface or moving a mouse. Once he has typed something he would like to be announced, the Text-to-Speech (TTS) software converts the text into audible speech in his iconic voice. By making this technology open source, anyone with a computer, motivation and an idea can make improvements. Prior to releasing the technology as open source, an inventor that is not associated with Intel would have to design the speech computer platform from scratch. Now, with all of Intel’s hard work out in the open for all to see, small improvements to the technology can be made without having to start again. This should increase the rate of improvement of the software and hardware, which will result in better quality and more accessible technology. People can now design accompanying software programs that can be added to the existing software or take the current design and create something more efficient or effective. This opens a world of possibilities for those with physical disabilities, especially those with ALS. Over three million people are affected by Motor Neuron Disease and Quadriplegia throughout the world. This technology could be adapted to allow many of them to communicate. Different functions can be activated by touch, blinking, eyebrow movements and other subtleties that can sensed by an infrared sensor.   . In 2004, Stephen Hawking chose NeoSpeech to produce his new voice. Using our VoiceText™ Text-to-Speech Software, Professor Hawking could communicate clearly with the outside world.   .  Since then, there have been vast improvements in TTS technology, but we are looking forward to seeing the exciting developments that come out of making Professor Hawking’s speech system open source. Are you excited to see the developments that come from making this technology open source? Do you know anyone who is working on speech technology project? We’d love to hear all about it! Contact    .",neoadmin,3
2015-08-19,http://blog.neospeech.com/2015/08/19/neospeech-text-to-speech-and-feeding-forward/,Can Text-to-Speech be used to End Hunger? NeoSpeech partners with Feeding Forward to realize this dreamIntroducing Feeding ForwardWhat part does NeoSpeech’s Text-to-Speech play?How does this help Feeding Forward?Learn More,"At NeoSpeech, we believe we have a duty to help those in need, both locally and nationally. Earlier in the year, we announced our partnership with the Military Freedom Fund, providing them with free Text-to-Speech software to help recognize the bravery of American soldiers and veterans. Now, our Text-to-Speech (TTS) software is being used to help provide meals to the hungry people of the Bay Area. The unfortunate truth is that 1 in 6 Americans do not know where their next meal is going to come from. Meanwhile, companies all around the USA are offering their employees meals, many of which go to waste. So, how can we make sure that food doesn’t go to waste? This is where NeoSpeech and Feeding Forward step in. Feeding Forward is a non-profit that takes excess food from donor companies, and with the help of our Text-to-Speech, delivers it to shelters and other recipient organizations in the Bay Area. They make it as easy as possible for you to donate food using their mobile app and will send you pictures of the positive difference you have made to people’s lives once the food has been delivered. Prior to this partnership, Feeding Forward had to call every shelter individually to announce that they had food available, which took up a lot of time and resources. Now, they can inform multiple shelters at once using high quality, dynamic text-to-speech solutions from NeoSpeech. Interestingly, one of Feeding Forward’s biggest challenges is giving away the food they receive. There are numerous government regulations about what food can be provided to which shelters, along with individual rules and regulations from the recipient organizations themselves. Some recipients will only accept food if it is gluten free or if it is delivered between certain hours, for example. Feeding Forward uses an algorithm to determine which shelter is most likely to accept the food they have been offered within a given timeframe and geographic area. NeoSpeech’s Text-to-Speech then automatically calls the best suited recipient organization and informs them of the specifics of the food and delivery time. Each phone call is personalized using NeoSpeech’s dynamic TTS. If the recipient organization rejects the food, the next best recipient organization is automatically called until an accepting host will take the food. Having free access to our TTS software has multiple positive effects on Feeding Forward: We had a chance to sit down with Reece Soltani, the Director of Strategic Initiatives at Feeding Forward to discuss the partnership and the opportunities that it opens up. “NeoSpeech is an integral part of our operations” says Soltani.  “Their software has allowed us to keep up with demand efficiently, expand beyond our staffing capacity through their language options to serve more communities in need, and engage our various stakeholders professionally.” Our Text-to-Speech has so many applications, from IVR (Interactive Voice Response) and CTI (Computer Telephony Integration) to e-learning platforms and more. As this article shows, our TTS can even be used to help feed the hungry!       .",neoadmin,5
2015-08-31,http://blog.neospeech.com/2015/08/31/male-canadian-french-voice-coming-soon/,"Leo, Our First Male Canadian French Voice – Coming Soon!So, tell me more about Leo’s VoiceChloe, our Female Canadian French VoiceHow long did it take to create these synthesized voices?","We are excited to announce the release of our first Male Canadian French voice, Leo, over the next few months! Leo joins the NeoSpeech team alongside Chloe, our female Canadian French voice. Together, they make an impressive team. Leo is so new that he is not even a demo on our website yet. But we couldn’t wait to let you know!  Leo’s voice has already been added to our Windows Text-to-Speech Engine packages. In October, we will have Leo’s voice available in our Text-to-Speech (TTS) Sever and Linux packages. In November, we aim to have a working demo on our website so that you can try Leo’s voice for free. In late November and December, we will be releasing iOS and Android compatible versions of Leo’s voice. By the end of the year, Leo’s voice will be fully available in all our products. f     f Leo’s voice is beautifully smooth with all the allure of a French accent. He has the ideal balance of a friendly and professional tone, which makes him perfect for IVR (Interactive Voice Response) and CTI (Computer Telephony Integration). Alternatively, Leo would make a brilliant personal voice, if you are looking for a new male French voice or just want to hear how fantastic Canadian French can sound as Leo reads your latest essay out loud. Leo can handle any text, long or short, with ease. As if his voice wasn’t enough to pull you in, Leo comes with a comprehensive customizable dictionary that enables you to customize the way he says certain words or phrases. This allows you to add industry-specific jargon or slang to your audio or add emphasis on certain sections of your sentence. This is a pretty cool feature, if we do say so ourselves. NeoSpeech introduced our first Canadian French voice, Chloe, in late 2012. While Chloe has done an excellent job of catering to our French market, we thought it was time to give her a male counterpart to offer a full range of Canadian French options. Chloe’s voice is very clear and comforting, making her ideal for e-learning applications. She would be especially well-suited to language learning and translation platforms where a precise yet friendly voice is required.   . Leo and Chloe are both USS (Unit Selection Synthesis) based synthesized voices; a technique which is known for producing the highest quality text-to-speech voices. After a year of hard work, NeoSpeech’s speech engineers are proud to be able to offer some of the highest quality Canadian French TTS voices available on the market. To us, this year is a worthwhile investment as we strive to produce consistently excellent quality and smooth synthesized voices.   h     f Are you interested in adding Leo’s voice to your next application or using him as a personal voice? Email us at   to get put on the list and emailed as soon as the Leo voice becomes available in your preferred package!       .",neoadmin,6
2015-08-24,http://blog.neospeech.com/2015/08/24/which-speech-synthesis-technique-is-better/,"HTS vs. USS: Which Speech Synthesis Technique is Better?
","By now you probably know about Text-to-Speech and how it works (if not, feel free to check out our What is Text-to-Speech and How Does it Work post to learn more). You also know that there are 2 main types of speech synthesis techniques – HTS (HMM based speech system) and USS (Unit Selection Synthesis). So, the next big question on everyone’s mind is  Here at NeoSpeech, we use USS because it is considered to produce the most natural sounding synthesized speech. But HTS has its merits too, such as a much shorter recording time and a smaller database. Let’s dive into examining these two techniques to find out the pros and cons of each one and help answer the question:  Let’s start with USS. As I said before, USS produces . This means that USS produces synthesized voices that sound the most human-like, the most realistic, and the most pleasant to listen to. This is the main reason that NeoSpeech uses USS – because no one wants to listen to a robotic voice that sounds like it is from an 80’s movie! In basic terms, USS involves inviting the voice actor in, recording hours and hours of audio, categorizing the actor’s speech into linguistic segments (e.g. words, phrases, and morphemes) and then putting all this information into a huge database. Our Text-to-Speech engine can then search this database for speech units that match the text you’ve typed in, concatenate them together and produce your audio file. . Let’s take a look at the main advantages and disadvantages of USS to get a grasp on whether this technique is the best approach for speech synthesis or if its main competitor, HTS, would win the fight. Great! Now we know a bit more about the pros and cons of USS, but what about its competitor, HTS? What does HTS bring to the table? HMM based speech synthesis system (also known as HTS) is a totally different ball game. HMM is a statistical parametric synthesis technique. The model is described as being parametric because it describes the speech using specific parameters, which is different to USS that uses stored speech segments. The statistical part comes in as the parameters are described, such as the mean and probability density. For those of you who are not majoring in linguistics or statistics,  , which the computer believes to be as similar as possible to the text you have written. All speech synthesis requires voice recordings from an actor, but HTS synthesis requires only 2-3 hours of recorded voice to be able to create a Text-to-Speech engine (compared to at least 20 hours for USS). The voice actor records 2-3 hours’ worth of speech. A database is developed that holds all the speech units from that voice actor. Then, the TTS engine is asked to turn text into speech. The HTS engine will then search and choose the most relevant statistical model to the text where statistical models were previously programmed through learning. The TTS engine finally uses the most correct statistical model to generate a synthetic audio file. Once again, let’s take a look at some of the pros and cons of HTS so we can gain the knowledge needed to decide if HTS or USS is the king of speech synthesis.   So, which one is better?  USS takes a lot of time – hours and hours of recording the voice actor’s speech and then hours and hours of computer programming. It can take several months or even a year to produce a truly high quality Text-To-Speech engine. However, it produces higher quality audio than HTS based synthesis. The HTS development process can be faster and cheaper than USS but the TTS engine requires more computation with more data quantity to process. In other words, it generally requires more computing resources, resulting in slower data processing than USS. 
 In short, the answer is that  . If you want to develop a high quality text-to-speech engine with a voice that is smooth, natural and human-like then choose USS. But if you want to produce a TTS engine quickly and cheaply, then choose HTS. Regardless of which speech algorithm we choose at NeoSpeech, we focus on high quality, natural sounding voices by realizing the best of the algorithms through our long R&D experiences and practices. NeoSpeech is now developing HTS-based emotional voices, speaker adaptation, and other applications. Leveraging our experiences and expertise, we are very close to releasing emotional voices. Stay tuned.  Looking to become a speech synthesis expert? Take a look at some of these academic articles on speech synthesis techniques:",neoadmin,6
2015-09-09,http://blog.neospeech.com/2015/09/09/advantages-of-text-to-speech/,The Advantages of Text-to-Speech,"Text-to-Speech software takes written text and transforms it into speech. This technology offers several benefits to consumers, businesses, personal users and educational institutions. Whether you’re a business looking for your next competitive edge or a personal user looking to learn a new language or to find your new voice, Text-to-Speech software can be implemented in a variety of ways to help you achieve your goals. In this post, we’ll dive into the numerous benefits of using text-to-speech for personal users, businesses and educational institutions. But first, here is a quick overview of some of the key advantages of using text-to-speech software, both for organizations and end users.  After a long day at work, it is time to give your eyes a break. Text-to-Speech allows you to relax and listen to the latest news, gossip, and trends without having to read a word. For example, apps like   readily transform recent entertainment, sport, health, business, technology and world news into audio content with a simple click of a button. 
 One of the essential elements of learning a new language is to hear the sounds, words and sentence flow of that language. Text-to-Speech allows you to see how a word is written in a different language and simultaneously hear how that word is pronounced. This can remove the need for an expensive tutor and give you the freedom to learn any time and anywhere. Text-to-Speech software can also be applied to many other online course materials and e-learning. Text-to-Speech enables students with learning disabilities, such as dyslexia, to excel. Using TTS, you can hear a sentence you have written out loud to ensure it says what you intended. This eliminates the stress of having to rely solely on visual cues to ensure your written content is correct. Text-to-Speech technology is also beneficial for people with difficulties pronouncing words. Our TTS has a range of speeds and volumes, which can be adjusted as you wish. This can be used to allow the user to slowly hear the specific pronunciation of a word so you can learn exactly how to say it. Text-to-Speech can also help people with physical disabilities too. A great example is enabling someone who is mute to communicate. TTS can be used on many devices, including smart phones and tablets, allowing TTS to be available at all times of the day. This enables people who are mute to communicate by simply typing in what they want to say and pressing a button.  Text-to-Speech can be utilized to help people with a variety of other physical disorders as well, such as Stephen Hawking who has Amyotrophic Lateral Sclerosis (ALS) and uses TTS to communicate his brilliant ideas. Text-to-Speech improves reading comprehension and speed. Not only can TTS help with pronunciation, but it can also be used for editing and proof-reading purposes. TTS can help highlight spelling and grammar issues in a written body of text. By hearing your text read aloud, you can hear that a sentence is too long or that a word has been spelt incorrectly. And just to make it as simple as possible, our TTS engine highlights the text that is being read aloud so you know exactly where you made a mistake in order to fix it. When we use as many of our 5 senses as possible when learning, we absorb more information and are much more likely to recall that information later. Text-to-Speech allows you to add an audio component to your learning so you can gain knowledge in multiple ways and solidify your learning. Text-to-Speech takes the hassle out of proofreading and editing. NeoSpeech’s TTS enables you to hear your text aloud so you can easily identify grammatical and spelling mistakes. It also highlights the text that is being spoken so you can pinpoint the exact word or sentence that needs changing. By making your website read-only, anyone with learning or reading-related disabilities may get frustrated and simply decide not to work with you. You could also be alienating people on their mobile phones who do not want to strain their eyes. By adding Text-to-Speech to your application, you can differentiate yourself from the competition by offering consumable content for all your users. Some consumers may want to read your content; others may prefer to listen to your content, and some consumers will want to do both. Give your customers the freedom to choose how they learn about and interact with your business by adding TTS functionality to your applications. Some students learn best with audio content, some learn better with written content, but comprehension and understanding are best when students learn in both ways. Text-to-Speech helps improve word recognition skills and increases their pronunciation capabilities. Students who use TTS have improved reading compression, accuracy, and ability to recall information. Are you interested in using text-to-speech technology in your everyday life? Or perhaps your organization is interested in integrating TTS into your next product, service or class? We’d love to help! If you have any questions or want to chat about more about Text-to-Speech, feel free to fill out our   form and our friendly sales team will be happy to help. Want to learn more about Text-to-Speech? Head on over to our   blog series to find out everything you need to know about TTS software. 
 Or, if you’d like to see some more specific applications of Text-to-Speech, check out our   page to see the full range of products and services that we can integrate NeoSpeech’s high quality text-to-speech into.",neoadmin,9
